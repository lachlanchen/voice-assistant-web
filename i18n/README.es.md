[English](../README.md) Â· [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](README.ar.md) Â· [EspaÃ±ol](README.es.md) Â· [FranÃ§ais](README.fr.md) Â· [æ—¥æœ¬èª](README.ja.md) Â· [í•œêµ­ì–´](README.ko.md) Â· [Tiáº¿ng Viá»‡t](README.vi.md) Â· [ä¸­æ–‡ (ç®€ä½“)](README.zh-Hans.md) Â· [ä¸­æ–‡ï¼ˆç¹é«”ï¼‰](README.zh-Hant.md) Â· [Deutsch](README.de.md) Â· [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README.ru.md)


<a name="readme-top"></a>

<br />
<div align="center">

<h3 align="center">Dale la bienvenida a Aura ğŸ‘‹</h3>

<p align="center">
Aura es un asistente de voz inteligente optimizado para respuestas de baja latencia. Usa Vercel Edge Functions, reconocimiento de voz Whisper, GPT-4o y streaming TTS de ElevenLabs.
<br />
<br />
<a href="https://voice.julianschoen.co">Ver demo</a>
Â·
<a href="https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=bug&projects=&template=bug_report.md&title=">Reportar error</a>
Â·
<a href="https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.md&title=">Solicitar funcionalidad</a>
</p>

<p align="center">
<a href="https://github.com/ntegrals/aura-voice"><img alt="Repo" src="https://img.shields.io/badge/GitHub-ntegrals%2Faura--voice-181717?logo=github" /></a>
<a href="https://nextjs.org/"><img alt="Next.js" src="https://img.shields.io/badge/Next.js-13.4.13-black?logo=next.js" /></a>
<a href="https://www.typescriptlang.org/"><img alt="TypeScript" src="https://img.shields.io/badge/TypeScript-5.1-3178C6?logo=typescript&logoColor=white" /></a>
<a href="https://openai.com/"><img alt="OpenAI" src="https://img.shields.io/badge/OpenAI-GPT--4o%20%2B%20Whisper-10A37F" /></a>
<a href="https://elevenlabs.io/"><img alt="ElevenLabs" src="https://img.shields.io/badge/ElevenLabs-TTS%20Streaming-222222" /></a>
<a href="https://vercel.com/"><img alt="Vercel Edge" src="https://img.shields.io/badge/Vercel-Edge%20Runtime-000000?logo=vercel" /></a>
<a href="./LICENCE"><img alt="License" src="https://img.shields.io/badge/License-MIT-22C55E.svg" /></a>
</p>

</div>

<a href="https://github.com/ntegrals/aura-voice">
<img src=".assets//header.png" alt="Logo">
</a>

## Tabla de contenidos

- [DescripciÃ³n general](#descripciÃ³n-general)
- [CaracterÃ­sticas](#caracterÃ­sticas)
- [Demo](#demo)
- [MotivaciÃ³n](#motivaciÃ³n)
- [Reflexiones sobre latencia y experiencia de usuario](#reflexiones-sobre-latencia-y-experiencia-de-usuario)
- [Arquitectura](#arquitectura)
- [Estructura del proyecto](#estructura-del-proyecto)
- [Requisitos previos](#requisitos-previos)
- [InstalaciÃ³n](#instalaciÃ³n)
- [ConfiguraciÃ³n](#configuraciÃ³n)
- [Uso](#uso)
- [Ejemplos de API](#ejemplos-de-api)
- [Notas de desarrollo](#notas-de-desarrollo)
- [ResoluciÃ³n de problemas](#resoluciÃ³n-de-problemas)
- [Hoja de ruta](#hoja-de-ruta)
- [Contribuir](#contribuir)
- [Contacto](#contacto)
- [Descargo de responsabilidad](#descargo-de-responsabilidad)
- [Licencia](#licencia)

## DescripciÃ³n general

Aura es un asistente de voz en el navegador, similar a Siri, creado con Next.js (App Router) y TypeScript.

### Resumen rÃ¡pido

| Ãrea | Detalles |
| --- | --- |
| Objetivo principal | InteracciÃ³n de voz web rÃ¡pida, prÃ¡ctica y de baja latencia |
| Modelo de ejecuciÃ³n | Captura en navegador + rutas API en servidor + endpoint de chat en Edge |
| Voz a texto | OpenAI Whisper (`whisper-1`) |
| Modelo del asistente | OpenAI GPT-4o |
| Texto a voz | ReproducciÃ³n en streaming de ElevenLabs en el navegador |

El ciclo de interacciÃ³n es:

1. Capturar audio del micrÃ³fono en el navegador.
2. Transcribir la voz con OpenAI Whisper (`whisper-1`).
3. Generar una respuesta concisa con OpenAI GPT-4o.
4. Transmitir audio sintetizado de vuelta al usuario mediante ElevenLabs.

El proyecto estÃ¡ optimizado para una UX prÃ¡ctica de baja latencia, con retroalimentaciÃ³n visual mientras el asistente estÃ¡ escuchando o pensando.

## CaracterÃ­sticas

âœ… Un asistente de voz tipo Siri dentro de tu navegador  
âœ… Optimizado para respuestas de baja latencia  
âœ… Construido con OpenAI, reconocimiento de voz Whisper y ElevenLabs

Detalles adicionales de implementaciÃ³n:

- Next.js 13 App Router con TypeScript.
- Endpoint de chat en Edge runtime (`/api/chat`).
- Feedback de interacciÃ³n con toasts (permiso de micrÃ³fono, escuchando, pensando).
- BotÃ³n animado del asistente con reproducciÃ³n TTS en streaming.
- Override opcional de la base URL de OpenAI para configuraciones con proxy/gateway autoalojado.

## Demo

Puedes probar Aura aquÃ­: [https://voice.julianschoen.co](https://voice.julianschoen.co)

## MotivaciÃ³n

Los asistentes de voz se han vuelto parte integral de la vida diaria: telÃ©fonos, coches, hogares y mÃ¡s. Llevar esa experiencia a la web con buena capacidad de respuesta histÃ³ricamente ha sido difÃ­cil.

Hasta hace poco, el principal problema de los asistentes de voz en la web era la latencia. Tardaba demasiado enviar audio al servidor, generar una respuesta con un LLM y volver a transmitir voz. Avances recientes de OpenAI, ElevenLabs y Vercel hicieron posible construir un asistente de voz lo bastante rÃ¡pido como para que sea prÃ¡ctico en la web.

Este repositorio busca ser un lugar de referencia para quienes quieren construir su propio asistente de voz y entender los tradeoffs en implementaciones reales.

## Reflexiones sobre latencia y experiencia de usuario

La latencia es el factor mÃ¡s importante para una buena UX de voz. Actualmente hay tres contribuyentes principales:

- Tiempo de transcripciÃ³n (reconocimiento de voz Whisper).
- Tiempo de generaciÃ³n de respuesta (GPT-4o Mini en las notas originales del proyecto).
- Tiempo de streaming de sÃ­ntesis de voz (TTS de ElevenLabs).

SegÃºn notas de pruebas prÃ¡cticas, la generaciÃ³n de voz suele tardar mÃ¡s y es la menos predecible, especialmente para respuestas largas.

Una posible estrategia de mitigaciÃ³n es dividir la respuesta en varias partes y transmitirlas una tras otra. Esto permite que el usuario empiece a escuchar antes, mientras el resto aÃºn se estÃ¡ generando. TodavÃ­a no estÃ¡ implementado, pero es una direcciÃ³n prometedora.

Otro concepto clave es el tiempo de espera percibido. Incluso cuando la latencia total es fija, los usuarios toleran mejor la espera si reciben feedback inmediato. El proyecto actualmente incluye una notificaciÃ³n de "thinking" durante el procesamiento para mejorar la sensaciÃ³n de respuesta.

## Arquitectura

```text
Browser (MediaRecorder)
  -> POST /api/speechToText (OpenAI Whisper transcription)
  -> POST /api/chat (OpenAI GPT-4o, Edge runtime)
  -> ElevenLabs TTS stream playback in browser (AudioContext)
```

Archivos clave:

- `src/components/AssistantButton/AssistantButton.tsx`: estado de grabaciÃ³n, orquestaciÃ³n de solicitudes y reproducciÃ³n.
- `src/app/api/speechToText/route.ts`: audio base64 -> `/tmp/input.webm` -> transcripciÃ³n con Whisper.
- `src/app/api/chat/route.ts`: chat completion vÃ­a OpenAI.
- `src/app/page.tsx`: interfaz orientada a escritorio y mensaje alternativo para mÃ³vil.

## Estructura del proyecto

```text
voice-assistant-web/
â”œâ”€ README.md
â”œâ”€ .env.example
â”œâ”€ package.json
â”œâ”€ LICENCE
â”œâ”€ CONTRIBUTING.md
â”œâ”€ CODE_OF_CONDUCT.md
â”œâ”€ .assets/
â”‚  â”œâ”€ header.png
â”‚  â””â”€ buymeacoffee.png
â”œâ”€ i18n/
â”œâ”€ public/
â”‚  â”œâ”€ font2.png
â”‚  â”œâ”€ favicon.ico
â”‚  â”œâ”€ next.svg
â”‚  â””â”€ vercel.svg
â””â”€ src/
   â”œâ”€ app/
   â”‚  â”œâ”€ page.tsx
   â”‚  â”œâ”€ layout.tsx
   â”‚  â”œâ”€ globals.css
   â”‚  â”œâ”€ button.css
   â”‚  â””â”€ api/
   â”‚     â”œâ”€ chat/route.ts
   â”‚     â””â”€ speechToText/route.ts
   â””â”€ components/
      â””â”€ AssistantButton/
         â””â”€ AssistantButton.tsx
```

## Requisitos previos

- Node.js 18+ (recomendado: Node.js 18.17+ o 20 LTS para Next.js 13).
- npm (el proyecto usa `package-lock.json`).
- Clave API de OpenAI.
- Clave API y voice ID de ElevenLabs.
- Un navegador de escritorio con acceso a micrÃ³fono (la UX mÃ³vil actualmente estÃ¡ limitada por diseÃ±o).

## InstalaciÃ³n

1. Clona el repositorio:

```sh
git clone https://github.com/ntegrals/aura-voice
```

2. ObtÃ©n las API keys en [https://openai.com/](https://openai.com/) y [https://elevenlabs.com/](https://elevenlabs.com/).

Copia el archivo `.env.example` a `.env.local` y agrega tus claves:

```sh
cp .env.example .env.local
```

```sh
OPENAI_API_KEY="YOUR OPENAI API KEY"
OPENAI_BASE_URL=(Optional)
NEXT_PUBLIC_ELEVENLABS_API_KEY="YOUR ELEVENLABS API KEY"
NEXT_PUBLIC_ELEVENLABS_VOICE_ID="YOUR ELEVENLABS VOICE ID"
```

3. Instala las dependencias:

```sh
npm install
```

4. Ejecuta la app en local:

```sh
npm run dev
```

5. Despliega en Vercel:

Este proyecto es compatible con el flujo de despliegue estÃ¡ndar de Vercel para Next.js.

## ConfiguraciÃ³n

Variables de entorno utilizadas por este proyecto:

| Variable | Requerida | DescripciÃ³n |
| --- | --- | --- |
| `OPENAI_API_KEY` | SÃ­ | API key usada para la transcripciÃ³n con Whisper y el chat completion con GPT. |
| `OPENAI_BASE_URL` | No | Override opcional de la URL base de la API de OpenAI (proxy/gateway). |
| `NEXT_PUBLIC_ELEVENLABS_API_KEY` | SÃ­ | API key de ElevenLabs usada en la solicitud TTS del lado del navegador. |
| `NEXT_PUBLIC_ELEVENLABS_VOICE_ID` | SÃ­ | Voice ID de ElevenLabs para la sÃ­ntesis TTS. |

Notas:

- Las variables `NEXT_PUBLIC_*` se exponen al cliente segÃºn las convenciones de Next.js.
- `speechToText` actualmente escribe audio temporal en `/tmp/input.webm` antes de transcribir.

## Uso

1. Abre la app en un navegador de escritorio.
2. Haz clic una vez en el orbe del asistente y concede permisos de micrÃ³fono.
3. Haz clic de nuevo para comenzar a grabar y luego otra vez para detener y enviar.
4. Aura transcribe tu entrada, genera una respuesta y luego reproduce voz sintetizada.

Scripts locales:

```sh
npm run dev
npm run build
npm run start
npm run lint
```

## Ejemplos de API

Estos ejemplos son Ãºtiles para depurar rutas API locales.

### `POST /api/speechToText`

```bash
curl -X POST http://localhost:3000/api/speechToText \
  -H "Content-Type: application/json" \
  -d '{"audio":"<base64-webm-audio>"}'
```

Forma de respuesta esperada:

```json
{
  "result": "transcribed text"
}
```

### `POST /api/chat`

```bash
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hello Aura"}]}'
```

Forma de respuesta esperada:

```json
"Assistant response text"
```

## Notas de desarrollo

- La ruta de chat estÃ¡ configurada para Edge runtime (`export const runtime = "edge"`).
- La ruta de Whisper se ejecuta en servidor y depende de acceso al sistema de archivos para almacenamiento temporal.
- Actualmente la UI muestra un mensaje alternativo en mÃ³vil en lugar de interacciÃ³n mÃ³vil completa.
- Se usan notificaciones tipo toast para exponer estados de permiso/escucha/procesamiento.
- El shaping actual del prompt pide respuestas concisas (`Your answer has to be as consise as possible.`).

## ResoluciÃ³n de problemas

### No aparece la solicitud de permiso del micrÃ³fono

- AsegÃºrate de que tu navegador permita acceso al micrÃ³fono para `localhost`.
- Usa HTTPS al probar en dominios que no sean localhost.

### No hay reproducciÃ³n de audio

- Verifica `NEXT_PUBLIC_ELEVENLABS_API_KEY` y `NEXT_PUBLIC_ELEVENLABS_VOICE_ID`.
- Comprueba restricciones de autoplay/audio-context del navegador (se requiere interacciÃ³n del usuario).

### API 500 desde `/api/speechToText`

- Confirma que `OPENAI_API_KEY` estÃ© configurada.
- Valida que la entrada contenga audio `webm` en base64 vÃ¡lido.

### API 500 desde `/api/chat`

- Confirma que `OPENAI_API_KEY` y el opcional `OPENAI_BASE_URL` sean correctos.
- Revisa la disponibilidad del modelo `gpt-4o` en tu cuenta de OpenAI.

### Latencia alta

- El tiempo de sÃ­ntesis TTS suele dominar la latencia end-to-end.
- MantÃ©n prompts concisos y considera dividir respuestas largas.

## Hoja de ruta

Posibles prÃ³ximas mejoras inferidas del cÃ³digo y las notas actuales:

- Soporte de interacciÃ³n mobile-first (reemplazar la limitaciÃ³n actual solo-escritorio).
- Streaming de respuestas parciales del asistente para reducir latencia percibida.
- Mejor UX de reintentos/errores en fallos de transcripciÃ³n y TTS.
- AÃ±adir pruebas automatizadas y checks de CI.
- Ampliar la documentaciÃ³n multilingÃ¼e en [`/i18n`](./i18n/).

## Contribuir

Las contribuciones son bienvenidas y se agradecen.

- Lee [CONTRIBUTING.md](./CONTRIBUTING.md) para conocer el flujo y las expectativas.
- Lee [CODE_OF_CONDUCT.md](./CODE_OF_CONDUCT.md) antes de participar.
- Abre issues para bugs o ideas de funcionalidades:
- Reporte de bug: [plantilla](https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=bug&projects=&template=bug_report.md&title=)
- Solicitud de funcionalidad: [plantilla](https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.md&title=)

## Contacto

Â¡Hola! Gracias por revisar y usar esta librerÃ­a. Si te interesa hablar sobre tu proyecto, necesitas mentorÃ­a, estÃ¡s pensando en contratarme o simplemente quieres conversar, encantado de hablar.

Puedes enviarme un email: `j.schoen@mail.com` o escribirme en Twitter: [@julianschoen](https://twitter.com/julianschoen)

Si quieres devolver algo, tengo una cuenta de Buy Me A Coffee:

<a href="https://www.buymeacoffee.com/ntegrals">
<img src=".assets/buymeacoffee.png" alt="buymeacoffee" width="192">
</a>

Gracias y que tengas un dÃ­a increÃ­ble ğŸ‘‹

## Descargo de responsabilidad

Voice Assistant es una aplicaciÃ³n experimental y se proporciona "tal cual", sin ninguna garantÃ­a, expresa o implÃ­cita. Al usar este software, aceptas asumir todos los riesgos asociados con su uso, incluidos, entre otros, pÃ©rdida de datos, fallos del sistema o cualquier otro problema que pueda surgir.

Los desarrolladores y contribuidores de este proyecto no aceptan ninguna responsabilidad por pÃ©rdidas, daÃ±os u otras consecuencias que puedan ocurrir como resultado del uso de este software. Eres el Ãºnico responsable de cualquier decisiÃ³n y acciÃ³n tomada en base a la informaciÃ³n proporcionada por Voice Assistant.

Ten en cuenta que el uso del modelo de lenguaje GPT-4 puede ser costoso debido al consumo de tokens. Al utilizar este proyecto, reconoces que eres responsable de supervisar y gestionar tu propio uso de tokens y los costos asociados. Se recomienda encarecidamente revisar tu uso de la API de OpenAI con regularidad y configurar lÃ­mites o alertas necesarios para evitar cargos inesperados.

Al usar Voice Assistant, aceptas indemnizar, defender y eximir de responsabilidad a los desarrolladores, contribuidores y cualquier parte afiliada frente a todas y cada una de las reclamaciones, daÃ±os, pÃ©rdidas, responsabilidades, costos y gastos (incluidos honorarios razonables de abogados) que surjan de tu uso de este software o del incumplimiento de estos tÃ©rminos.

<!-- LICENSE -->

## Licencia

Distribuido bajo la licencia MIT. Consulta `LICENSE` para mÃ¡s informaciÃ³n.

Nota del repositorio: actualmente este repositorio almacena el archivo de licencia como [`LICENCE`](./LICENCE).
