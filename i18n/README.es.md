[English](../README.md) ¬∑ [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](README.ar.md) ¬∑ [Espa√±ol](README.es.md) ¬∑ [Fran√ßais](README.fr.md) ¬∑ [Êó•Êú¨Ë™û](README.ja.md) ¬∑ [ÌïúÍµ≠Ïñ¥](README.ko.md) ¬∑ [Ti·∫øng Vi·ªát](README.vi.md) ¬∑ [‰∏≠Êñá (ÁÆÄ‰Ωì)](README.zh-Hans.md) ¬∑ [‰∏≠ÊñáÔºàÁπÅÈ´îÔºâ](README.zh-Hant.md) ¬∑ [Deutsch](README.de.md) ¬∑ [–†—É—Å—Å–∫–∏–π](README.ru.md)


[![LazyingArt banner](https://github.com/lachlanchen/lachlanchen/raw/main/figs/banner.png)](https://github.com/lachlanchen/lachlanchen/blob/main/figs/banner.png)

![GitHub last commit](https://img.shields.io/github/last-commit/ntegrals/aura-voice?style=for-the-badge&logo=github&logoColor=white&color=0EA5E9)
![Node.js](https://img.shields.io/badge/Node.js-18%2B-10B981?style=for-the-badge&logo=nodedotjs&logoColor=white)
![TypeScript](https://img.shields.io/badge/TypeScript-5.1-3178C6?style=for-the-badge&logo=typescript&logoColor=white)
![GitHub stars](https://img.shields.io/github/stars/ntegrals/aura-voice?style=for-the-badge&logo=github&logoColor=white&color=F59E0B)
![Open Issues](https://img.shields.io/github/issues/ntegrals/aura-voice?style=for-the-badge&logo=github&logoColor=white&color=EF4444)

<a name="readme-top"></a>

<br />
<div align="center">

# Aura

<h3 align="center">Conoce a Aura üëã</h3>

<p align="center">
Aura es un asistente de voz basado en navegador, parecido a Siri, optimizado para respuestas de baja latencia. Utiliza Vercel Edge Functions, reconocimiento de voz Whisper, GPT-4o y streaming de TTS de ElevenLabs.
<br />
<br />
<a href="https://voice.julianschoen.co"><img src="https://img.shields.io/badge/‚ñ∂_Live_Demo-0EA5E9?style=for-the-badge&logo=googlechrome&logoColor=white" alt="Live Demo"/></a>
<a href="https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=bug&projects=&template=bug_report.md&title="><img src="https://img.shields.io/badge/üêû_Report_Bug-F43F5E?style=for-the-badge&logo=github&logoColor=white" alt="Report Bug"/></a>
<a href="https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.md&title="><img src="https://img.shields.io/badge/üí°_Request_Feature-22C55E?style=for-the-badge&logo=github&logoColor=white" alt="Request Feature"/></a>
</p>

<p align="center">
<a href="https://github.com/ntegrals/aura-voice"><img alt="Repo" src="https://img.shields.io/badge/GitHub-ntegrals%2Faura--voice-181717?logo=github" /></a>
<a href="https://nextjs.org/"><img alt="Next.js" src="https://img.shields.io/badge/Next.js-13.4.13-black?logo=next.js" /></a>
<a href="https://www.typescriptlang.org/"><img alt="TypeScript" src="https://img.shields.io/badge/TypeScript-5.1-3178C6?logo=typescript&logoColor=white" /></a>
<a href="https://openai.com/"><img alt="OpenAI" src="https://img.shields.io/badge/OpenAI-GPT--4o%20%2B%20Whisper-10A37F" /></a>
<a href="https://elevenlabs.io/"><img alt="ElevenLabs" src="https://img.shields.io/badge/ElevenLabs-TTS%20Streaming-222222" /></a>
<a href="https://vercel.com/"><img alt="Vercel Edge" src="https://img.shields.io/badge/Vercel-Edge%20Runtime-000000?logo=vercel" /></a>
<a href="./LICENCE"><img alt="License" src="https://img.shields.io/badge/License-MIT-22C55E.svg" /></a>
</p>

</div>

<a href="https://github.com/ntegrals/aura-voice">
<img src=".assets//header.png" alt="Logo">
</a>

## √çndice

- [üìå Visi√≥n general](#visi√≥n-general)
- [‚ú® Caracter√≠sticas](#caracter√≠sticas)
- [üé• Demo](#demo)
- [üß† Motivaci√≥n](#motivaci√≥n)
- [‚è±Ô∏è Ideas sobre latencia y experiencia de usuario](#ideas-sobre-latencia-y-experiencia-de-usuario)
- [üèóÔ∏è Arquitectura](#arquitectura)
- [üìÅ Estructura del proyecto](#estructura-del-proyecto)
- [‚úÖ Requisitos previos](#requisitos-previos)
- [üß∞ Instalaci√≥n](#instalaci√≥n)
- [‚öôÔ∏è Configuraci√≥n](#configuraci√≥n)
- [üß™ Uso](#uso)
- [üì¶ Ejemplos de API](#ejemplos-de-api)
- [üõ†Ô∏è Notas de desarrollo](#notas-de-desarrollo)
- [üßØ Soluci√≥n de problemas](#soluci√≥n-de-problemas)
- [üó∫Ô∏è Hoja de ruta](#hoja-de-ruta)
- [‚ù§Ô∏è Support](#%e2%9d%a4%ef%b8%8f-support)
- [üì¨ Contacto](#contacto)
- [‚ö†Ô∏è Descargo de responsabilidad](#descargo-de-responsabilidad)
- [üìÑ Licencia](#licencia)

## Visi√≥n general

Aura es un asistente de voz basado en el navegador, similar a Siri, construido con Next.js (App Router) y TypeScript.

### En un vistazo

| √Årea | Detalles |
| --- | --- |
| Objetivo principal | Interacci√≥n de voz r√°pida, pr√°ctica y de baja latencia en la web |
| Modelo de ejecuci√≥n | Captura en el navegador + rutas API del servidor + endpoint de chat en Edge |
| Voz a texto | OpenAI Whisper (`whisper-1`) |
| Modelo del asistente | OpenAI GPT-4o |
| Texto a voz | Reproducci√≥n con streaming de ElevenLabs en el navegador |

El ciclo de interacci√≥n es:

1. Capturar el audio del micr√≥fono en el navegador.
2. Transcribir el habla con OpenAI Whisper (`whisper-1`).
3. Generar una respuesta concisa con OpenAI GPT-4o.
4. Enviar audio sintetizado al usuario mediante ElevenLabs.

El proyecto est√° optimizado para una experiencia de usuario de baja latencia centrada en el uso real, con retroalimentaci√≥n visual mientras el asistente escucha o est√° procesando.

### Resumen visual

| Etapa | Intenci√≥n |
| --- | --- |
| üéôÔ∏è Captura | Captura de audio en el navegador + estados de interfaz conscientes de permisos |
| üß† Procesamiento | Transcripci√≥n con Whisper + generaci√≥n de respuesta de GPT-4o |
| üîâ Entrega | Reproducci√≥n de ElevenLabs con streaming y estado de progreso |

## Caracter√≠sticas

| Capacidad | Qu√© significa |
| --- | --- |
| ‚úÖ Asistente de voz tipo Siri | Interacci√≥n completa de voz de entrada y salida en una interfaz web simple |
| ‚ö° Flujo de trabajo de baja latencia | Bucle de captura, transcripci√≥n, completado y reproducci√≥n optimizado |
| üß† Stack LLM + TTS | OpenAI Whisper, GPT-4o y s√≠ntesis en streaming de ElevenLabs |
| üß© Arquitectura extensible | Cambia el endpoint del modelo o proveedor de voz con ajustes a nivel de proyecto |

Detalles adicionales de implementaci√≥n:

| √Årea de enfoque | Comportamiento actual |
| --- | --- |
| Framework | Next.js 13 App Router con TypeScript |
| Tiempo de ejecuci√≥n de API | Endpoint de chat con runtime Edge (`/api/chat`) |
| Retroalimentaci√≥n UX | Notificaciones toast para permisos de micr√≥fono, escucha y estado de procesamiento |
| Interfaz de interacci√≥n | Bot√≥n animado del asistente con reproducci√≥n TTS en streaming |
| Red | Sobrescritura opcional de la URL base de OpenAI para setups de proxy/self-hosted gateway |

## Demo

Puedes probar Aura aqu√≠: [https://voice.julianschoen.co](https://voice.julianschoen.co)

## Motivaci√≥n

Los asistentes de voz se han vuelto parte integral de la vida diaria: tel√©fonos, autos, hogares y m√°s. Llevar esa experiencia a la web con buena capacidad de respuesta hist√≥ricamente fue dif√≠cil.

Hasta hace poco, el principal problema con los asistentes de voz en la web era la latencia. Tardaba demasiado enviar el audio al servidor, generar una completion de LLM y devolver el habla. Los avances recientes de OpenAI, ElevenLabs y Vercel han hecho posible construir un asistente de voz suficientemente r√°pido como para ser pr√°ctico en la web.

Este repositorio busca ser un punto de referencia para quienes desean construir su propio asistente de voz y entender los compromisos en implementaciones reales.

## Ideas sobre latencia y experiencia de usuario

La latencia es el factor m√°s importante para una buena experiencia de voz. Actualmente hay tres contribuyentes principales:

- Tiempo de transcripci√≥n (reconocimiento de voz Whisper).
- Tiempo de generaci√≥n de respuesta (GPT-4o Mini en las notas originales del proyecto).
- Tiempo de streaming de s√≠ntesis de voz (ElevenLabs TTS).

Por experiencia pr√°ctica, la generaci√≥n de voz suele llevar m√°s tiempo y ser lo menos predecible, especialmente en respuestas m√°s largas.

Una posible estrategia de mitigaci√≥n es dividir la respuesta en varias partes y transmitirlas una detr√°s de otra. Esto permite que el usuario empiece a escuchar antes, mientras el resto a√∫n se est√° generando. A√∫n no est√° implementado, pero es una direcci√≥n prometedora.

Otro concepto clave es el tiempo de espera percibido. Incluso con una latencia total fija, los usuarios toleran mejor los retrasos cuando reciben retroalimentaci√≥n inmediata. El proyecto incluye actualmente un aviso de "pensando" durante el procesamiento para mejorar la sensaci√≥n de respuesta.

## Arquitectura

```text
Browser (MediaRecorder)
  -> POST /api/speechToText (OpenAI Whisper transcription)
  -> POST /api/chat (OpenAI GPT-4o, Edge runtime)
  -> ElevenLabs TTS stream playback in browser (AudioContext)
```

Archivos clave:

- `src/components/AssistantButton/AssistantButton.tsx`: estado de grabaci√≥n, orquestaci√≥n de solicitudes y reproducci√≥n.
- `src/app/api/speechToText/route.ts`: audio base64 -> `/tmp/input.webm` -> transcripci√≥n Whisper.
- `src/app/api/chat/route.ts`: generaci√≥n de completion de chat a trav√©s de OpenAI.
- `src/app/page.tsx`: interfaz orientada a escritorio y mensaje de fallback para m√≥viles.

## Estructura del proyecto

```text
voice-assistant-web/
‚îú‚îÄ README.md
‚îú‚îÄ .env.example
‚îú‚îÄ package.json
‚îú‚îÄ LICENCE
‚îú‚îÄ CONTRIBUTING.md
‚îú‚îÄ CODE_OF_CONDUCT.md
‚îú‚îÄ .assets/
‚îÇ  ‚îú‚îÄ header.png
‚îÇ  ‚îî‚îÄ buymeacoffee.png
‚îú‚îÄ i18n/
‚îú‚îÄ public/
‚îÇ  ‚îú‚îÄ font2.png
‚îÇ  ‚îú‚îÄ favicon.ico
‚îÇ  ‚îú‚îÄ next.svg
‚îÇ  ‚îî‚îÄ vercel.svg
‚îî‚îÄ src/
   ‚îú‚îÄ app/
   ‚îÇ  ‚îú‚îÄ page.tsx
   ‚îÇ  ‚îú‚îÄ layout.tsx
   ‚îÇ  ‚îú‚îÄ globals.css
   ‚îÇ  ‚îú‚îÄ button.css
   ‚îÇ  ‚îî‚îÄ api/
   ‚îÇ     ‚îú‚îÄ chat/route.ts
   ‚îÇ     ‚îî‚îÄ speechToText/route.ts
   ‚îî‚îÄ components/
      ‚îî‚îÄ AssistantButton/
         ‚îî‚îÄ AssistantButton.tsx
```

## Requisitos previos

| Requisito | Detalles |
| --- | --- |
| Node.js | 18+ (recomendado: Node.js 18.17+ o 20 LTS para Next.js 13) |
| Gestor de paquetes | npm (el proyecto usa `package-lock.json`) |
| Acceso a API | Clave API de OpenAI |
| Acceso a TTS | Clave API de ElevenLabs e ID de voz |
| Cliente | Navegador de escritorio con acceso al micr√≥fono (la experiencia m√≥vil hoy prioriza escritorio) |

## Instalaci√≥n

1. Clona el repositorio:

```sh
git clone https://github.com/ntegrals/aura-voice
```

2. Copia la plantilla de entorno y edita los valores:

```sh
cp .env.example .env.local
```

```sh
OPENAI_API_KEY="YOUR OPENAI API KEY"
OPENAI_BASE_URL="" # Optional
NEXT_PUBLIC_ELEVENLABS_API_KEY="YOUR ELEVENLABS API KEY"
NEXT_PUBLIC_ELEVENLABS_VOICE_ID="YOUR ELEVENLABS VOICE ID"
```

3. Instala dependencias:

```sh
npm install
```

4. Ejecuta la app en local:

```sh
npm run dev
```

5. Abre la app en `http://localhost:3000`.

Suponemos que si pruebas acceso al micr√≥fono en dominios no locales, normalmente se requiere HTTPS.

6. Despliega en Vercel:

Este proyecto sigue un flujo de despliegue est√°ndar de Next.js. Usa la configuraci√≥n de importaci√≥n por defecto de Vercel y define las mismas variables de entorno en tu proyecto.

## Configuraci√≥n

Variables de entorno usadas por este proyecto:

| Variable | Requerida | Descripci√≥n |
| --- | --- | --- |
| `OPENAI_API_KEY` | S√≠ | Clave API para transcripci√≥n Whisper y completion de chat de GPT. |
| `OPENAI_BASE_URL` | No | Sobrescritura opcional de la URL base de la API de OpenAI (proxy/gateway). |
| `NEXT_PUBLIC_ELEVENLABS_API_KEY` | S√≠ | Clave API de ElevenLabs usada en la solicitud TTS del lado del cliente. |
| `NEXT_PUBLIC_ELEVENLABS_VOICE_ID` | S√≠ | ID de voz de ElevenLabs para la s√≠ntesis TTS. |

Notas:

- Las variables `NEXT_PUBLIC_*` se exponen al cliente conforme a las convenciones de Next.js.
- `speechToText` actualmente guarda el audio temporal en `/tmp/input.webm` antes de la transcripci√≥n.

## Uso

1. Abre la app en un navegador de escritorio.
2. Haz clic una vez en el bot√≥n del asistente y concede permisos de micr√≥fono.
3. Haz clic de nuevo para iniciar la grabaci√≥n, y de nuevo para detenerla y enviarla.
4. Aura transcribe tu entrada, genera una respuesta y luego reproduce el audio sintetizado.

Scripts locales:

```sh
npm run dev
npm run build
npm run start
npm run lint
```

## Ejemplos de API

Estos ejemplos son √∫tiles para depurar las rutas API locales.

### `POST /api/speechToText`

```bash
curl -X POST http://localhost:3000/api/speechToText \
  -H "Content-Type: application/json" \
  -d '{"audio":"<base64-webm-audio>"}'
```

Forma esperada de respuesta:

```json
{
  "result": "transcribed text"
}
```

### `POST /api/chat`

```bash
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hello Aura"}]}'
```

Forma esperada de respuesta:

```json
"Assistant response text"
```

## Notas de desarrollo

- La ruta de chat est√° configurada para Edge runtime (`export const runtime = "edge"`).
- La ruta de Whisper se ejecuta del lado del servidor y depende del acceso al sistema de archivos para almacenamiento temporal.
- La UI actualmente muestra un mensaje de fallback para m√≥vil en lugar de una interacci√≥n m√≥vil completa.
- Se usan notificaciones toast para mostrar estados de permiso, escucha y procesamiento.
- El prompt actual pide respuestas concisas (`Your answer has to be as concise as possible.`).
- Los logs de runtime, la trazabilidad de solicitudes y el comportamiento de streaming no est√°n probados actualmente en CI (no hay suite de pruebas automatizada en este repositorio).

## Soluci√≥n de problemas

### üé§ La solicitud de permiso del micr√≥fono no aparece

- Aseg√∫rate de que tu navegador permita acceso al micr√≥fono para `localhost`.
- Usa HTTPS al probar en dominios que no sean localhost.

### üîà No se reproduce audio

- Verifica `NEXT_PUBLIC_ELEVENLABS_API_KEY` y `NEXT_PUBLIC_ELEVENLABS_VOICE_ID`.
- Comprueba las restricciones de autoplay/audio-context del navegador (se requiere interacci√≥n del usuario).

### üì° Error 500 de la API en `/api/speechToText`

- Confirma que `OPENAI_API_KEY` est√© configurada.
- Comprueba que la entrada incluya audio `webm` v√°lido codificado en base64.

### üì° Error 500 de la API en `/api/chat`

- Confirma que `OPENAI_API_KEY` y `OPENAI_BASE_URL` (opcional) sean correctas.
- Verifica la disponibilidad del modelo `gpt-4o` en tu cuenta de OpenAI.

### ‚è≥ Alta latencia

- El tiempo de s√≠ntesis de voz suele dominar la latencia de extremo a extremo.
- Mant√©n prompts concisos y considera dividir respuestas largas.

## Hoja de ruta

Mejoras potenciales inferidas del c√≥digo actual y las notas:

- Soporte de interacci√≥n con enfoque m√≥vil (sustituir la restricci√≥n actual orientada a escritorio).
- Streaming de respuestas parciales del asistente para reducir la latencia percibida.
- Mejorar la UX de reintentos y errores alrededor de fallos de transcripci√≥n y TTS.
- A√±adir pruebas automatizadas y comprobaciones de CI.
- Expandir la documentaci√≥n multiling√ºe en [`/i18n`](./i18n/).

## Contribuir

Las contribuciones son bienvenidas y apreciadas.

- Lee [CONTRIBUTING.md](./CONTRIBUTING.md) para conocer el flujo de trabajo y las expectativas.
- Lee [CODE_OF_CONDUCT.md](./CODE_OF_CONDUCT.md) antes de participar.
- Abre issues para reportar errores o proponer mejoras:
  - Informe de error: [template](https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=bug&projects=&template=bug_report.md&title=)
  - Solicitud de funci√≥n: [template](https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.md&title=)

## ‚ù§Ô∏è Support

| Donate | PayPal | Stripe |
| --- | --- | --- |
| [![Donate](https://camo.githubusercontent.com/24a4914f0b42c6f435f9e101621f1e52535b02c225764b2f6cc99416926004b7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d4c617a79696e674172742d3045413545393f7374796c653d666f722d7468652d6261646765266c6f676f3d6b6f2d6669266c6f676f436f6c6f723d7768697465)](https://chat.lazying.art/donate) | [![PayPal](https://camo.githubusercontent.com/d0f57e8b016517a4b06961b24d0ca87d62fdba16e18bbdb6aba28e978dc0ea21/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617950616c2d526f6e677a686f754368656e2d3030343537433f7374796c653d666f722d7468652d6261646765266c6f676f3d70617970616c266c6f676f436f6c6f723d7768697465)](https://paypal.me/RongzhouChen) | [![Stripe](https://camo.githubusercontent.com/1152dfe04b6943afe3a8d2953676749603fb9f95e24088c92c97a01a897b4942/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5374726970652d446f6e6174652d3633354246463f7374796c653d666f722d7468652d6261646765266c6f676f3d737472697065266c6f676f436f6c6f723d7768697465)](https://buy.stripe.com/aFadR8gIaflgfQV6T4fw400) |

## Contacto

¬°Hola! Gracias por mirar y usar esta librer√≠a. Si te interesa hablar de tu proyecto, necesitas mentor√≠a, quieres contratarme o simplemente charlar, estar√© encantado de hablar.

Puedes enviarme un correo a `j.schoen@mail.com` o escribirme en Twitter: [@julianschoen](https://twitter.com/julianschoen)

Si quieres agradecer, tengo una cuenta de Buy Me A Coffee:

<a href="https://www.buymeacoffee.com/ntegrals">
<img src=".assets/buymeacoffee.png" alt="buymeacoffee" width="192">
</a>

¬°Gracias y que tengas un gran d√≠a üëã

## Descargo de responsabilidad

Voice Assistant es una aplicaci√≥n experimental y se ofrece "tal cual" sin ninguna garant√≠a, expresa o impl√≠cita. Al usar este software, aceptas asumir todos los riesgos asociados con su uso, incluyendo, entre otros, p√©rdida de datos, fallos del sistema o cualquier otro problema que pueda surgir.

Los desarrolladores y colaboradores de este proyecto no aceptan ninguna responsabilidad o obligaci√≥n por p√©rdidas, da√±os u otras consecuencias que puedan derivarse del uso de este software. T√∫ eres el √∫nico responsable de cualquier decisi√≥n o acci√≥n basada en la informaci√≥n proporcionada por Voice Assistant.

Ten en cuenta que usar el modelo GPT-4 puede ser costoso por el consumo de tokens. Al utilizar este proyecto, reconoces que eres responsable de controlar y gestionar tu propio uso de tokens y costos asociados. Se recomienda encarecidamente revisar regularmente tu uso de la API de OpenAI y configurar l√≠mites o alertas para evitar cargos inesperados.

Al usar Voice Assistant, aceptas indemnizar, defender y eximir de responsabilidad a los desarrolladores, colaboradores y cualquier parte asociada por cualquier reclamaci√≥n, da√±o, p√©rdida, responsabilidad, costo y gasto (incluidos honorarios razonables de abogados) que surjan del uso de este software o de la violaci√≥n de estos t√©rminos.

<!-- LICENSE -->

## Licencia

Distribuido bajo la Licencia MIT. Consulta `LICENSE` para m√°s informaci√≥n.

Nota del repositorio: este repositorio actualmente guarda el archivo de licencia como [`LICENCE`](./LICENCE).
