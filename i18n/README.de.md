[English](../README.md) Â· [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](README.ar.md) Â· [EspaÃ±ol](README.es.md) Â· [FranÃ§ais](README.fr.md) Â· [æ—¥æœ¬èª](README.ja.md) Â· [í•œêµ­ì–´](README.ko.md) Â· [Tiáº¿ng Viá»‡t](README.vi.md) Â· [ä¸­æ–‡ (ç®€ä½“)](README.zh-Hans.md) Â· [ä¸­æ–‡ï¼ˆç¹é«”ï¼‰](README.zh-Hant.md) Â· [Deutsch](README.de.md) Â· [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README.ru.md)


<a name="readme-top"></a>

<br />
<div align="center">

<h3 align="center">Sag Hallo zu Aura ğŸ‘‹</h3>

<p align="center">
Aura ist ein intelligenter Sprachassistent, optimiert fÃ¼r Antworten mit niedriger Latenz. Er nutzt Vercel Edge Functions, Whisper-Spracherkennung, GPT-4o und ElevenLabs-TTS-Streaming.
<br />
<br />
<a href="https://voice.julianschoen.co">Demo ansehen</a>
Â·
<a href="https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=bug&projects=&template=bug_report.md&title=">Bug melden</a>
Â·
<a href="https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.md&title=">Feature anfragen</a>
</p>

<p align="center">
<a href="https://github.com/ntegrals/aura-voice"><img alt="Repo" src="https://img.shields.io/badge/GitHub-ntegrals%2Faura--voice-181717?logo=github" /></a>
<a href="https://nextjs.org/"><img alt="Next.js" src="https://img.shields.io/badge/Next.js-13.4.13-black?logo=next.js" /></a>
<a href="https://www.typescriptlang.org/"><img alt="TypeScript" src="https://img.shields.io/badge/TypeScript-5.1-3178C6?logo=typescript&logoColor=white" /></a>
<a href="https://openai.com/"><img alt="OpenAI" src="https://img.shields.io/badge/OpenAI-GPT--4o%20%2B%20Whisper-10A37F" /></a>
<a href="https://elevenlabs.io/"><img alt="ElevenLabs" src="https://img.shields.io/badge/ElevenLabs-TTS%20Streaming-222222" /></a>
<a href="https://vercel.com/"><img alt="Vercel Edge" src="https://img.shields.io/badge/Vercel-Edge%20Runtime-000000?logo=vercel" /></a>
<a href="./LICENCE"><img alt="Lizenz" src="https://img.shields.io/badge/License-MIT-22C55E.svg" /></a>
</p>

</div>

<a href="https://github.com/ntegrals/aura-voice">
<img src=".assets//header.png" alt="Logo">
</a>

## Inhaltsverzeichnis

- [Ãœberblick](#Ã¼berblick)
- [Features](#features)
- [Demo](#demo)
- [Motivation](#motivation)
- [Gedanken zu Latenz und Benutzererlebnis](#gedanken-zu-latenz-und-benutzererlebnis)
- [Architektur](#architektur)
- [Projektstruktur](#projektstruktur)
- [Voraussetzungen](#voraussetzungen)
- [Installation](#installation)
- [Konfiguration](#konfiguration)
- [Verwendung](#verwendung)
- [API-Beispiele](#api-beispiele)
- [Entwicklungshinweise](#entwicklungshinweise)
- [Fehlerbehebung](#fehlerbehebung)
- [Roadmap](#roadmap)
- [Mitwirken](#mitwirken)
- [Kontakt](#kontakt)
- [Haftungsausschluss](#haftungsausschluss)
- [Lizenz](#lizenz)

## Ãœberblick

Aura ist ein browserbasierter, Siri-Ã¤hnlicher Sprachassistent, der mit Next.js (App Router) und TypeScript entwickelt wurde.

### Auf einen Blick

| Bereich | Details |
| --- | --- |
| Hauptziel | Schnelle, praktische Sprachinteraktion mit niedriger Latenz im Web |
| Laufzeitmodell | Browser-Aufnahme + Server-API-Routen + Edge-Chat-Endpunkt |
| Speech-to-Text | OpenAI Whisper (`whisper-1`) |
| Assistentenmodell | OpenAI GPT-4o |
| Text-to-Speech | ElevenLabs-Streaming-Wiedergabe im Browser |

Der Interaktionszyklus ist:

1. Mikrofon-Audio im Browser erfassen.
2. Sprache mit OpenAI Whisper (`whisper-1`) transkribieren.
3. Eine prÃ¤gnante Antwort mit OpenAI GPT-4o erzeugen.
4. Synthetisiertes Audio per ElevenLabs an den Nutzer zurÃ¼ckstreamen.

Das Projekt ist auf eine praxisnahe UX mit niedriger Latenz optimiert, inklusive visuellem Feedback, wÃ¤hrend der Assistent zuhÃ¶rt oder nachdenkt.

## Features

âœ… Ein Siri-Ã¤hnlicher Sprachassistent direkt im Browser  
âœ… Optimiert fÃ¼r Antworten mit niedriger Latenz  
âœ… Entwickelt mit OpenAI, Whisper-Spracherkennung und ElevenLabs

ZusÃ¤tzliche Implementierungsdetails:

- Next.js 13 App Router mit TypeScript.
- Edge-Runtime-Chat-Endpunkt (`/api/chat`).
- Toast-basiertes Interaktionsfeedback (Mikrofonberechtigung, ZuhÃ¶ren, Nachdenken).
- Animierter Assistenten-Button mit Streaming-TTS-Wiedergabe.
- Optionale OpenAI-Base-URL-Ãœberschreibung fÃ¼r Proxy-/Self-Hosted-Gateway-Setups.

## Demo

Du kannst Aura hier testen: [https://voice.julianschoen.co](https://voice.julianschoen.co)

## Motivation

Sprachassistenten sind ein fester Bestandteil des Alltags geworden: auf Smartphones, im Auto, zu Hause und mehr. Diese Erfahrung mit guter Reaktionsgeschwindigkeit ins Web zu bringen, war historisch schwierig.

Bis vor Kurzem war Latenz das Hauptproblem von Sprachassistenten im Web. Das Senden von Audio an den Server, das Erzeugen einer LLM-Antwort und das ZurÃ¼ckstreamen von Sprache dauerten zu lange. JÃ¼ngste Fortschritte bei OpenAI, ElevenLabs und Vercel ermÃ¶glichen es, einen Sprachassistenten zu bauen, der im Web schnell genug fÃ¼r die Praxis ist.

Dieses Repository soll eine zentrale Anlaufstelle fÃ¼r alle sein, die ihren eigenen Sprachassistenten bauen und die Trade-offs realer Implementierungen verstehen wollen.

## Gedanken zu Latenz und Benutzererlebnis

Latenz ist der wichtigste Faktor fÃ¼r eine gute Voice-UX. Aktuell gibt es drei HauptbeitrÃ¤ge:

- Transkriptionszeit (Whisper-Spracherkennung).
- Zeit fÃ¼r die Antwortgenerierung (GPT-4o Mini in den ursprÃ¼nglichen Projektnotizen).
- Streaming-Zeit der Sprachsynthese (ElevenLabs TTS).

Aus praktischen Testnotizen geht hervor, dass die Spracherzeugung in der Regel am meisten Zeit kostet und am wenigsten vorhersagbar ist, insbesondere bei lÃ¤ngeren Antworten.

Eine mÃ¶gliche GegenmaÃŸnahme ist, die Antwort in mehrere Teile aufzuteilen und nacheinander zu streamen. So kann der Nutzer frÃ¼her mit dem ZuhÃ¶ren beginnen, wÃ¤hrend der Rest noch erzeugt wird. Das ist aktuell noch nicht implementiert, aber ein vielversprechender Ansatz.

Ein weiteres SchlÃ¼sselkonzept ist die wahrgenommene Wartezeit. Selbst wenn die Gesamtlatenz unverÃ¤ndert ist, tolerieren Nutzer VerzÃ¶gerungen besser, wenn sie sofortiges Feedback erhalten. Das Projekt enthÃ¤lt derzeit eine "thinking"-Benachrichtigung wÃ¤hrend der Verarbeitung, um die wahrgenommene Reaktionsgeschwindigkeit zu verbessern.

## Architektur

```text
Browser (MediaRecorder)
  -> POST /api/speechToText (OpenAI Whisper transcription)
  -> POST /api/chat (OpenAI GPT-4o, Edge runtime)
  -> ElevenLabs TTS stream playback in browser (AudioContext)
```

Wichtige Dateien:

- `src/components/AssistantButton/AssistantButton.tsx`: Aufnahmestatus, Request-Orchestrierung, Wiedergabe.
- `src/app/api/speechToText/route.ts`: base64-Audio -> `/tmp/input.webm` -> Whisper-Transkription.
- `src/app/api/chat/route.ts`: Chat-Completion via OpenAI.
- `src/app/page.tsx`: Desktop-First-OberflÃ¤che und Mobile-Fallback-Nachricht.

## Projektstruktur

```text
voice-assistant-web/
â”œâ”€ README.md
â”œâ”€ .env.example
â”œâ”€ package.json
â”œâ”€ LICENCE
â”œâ”€ CONTRIBUTING.md
â”œâ”€ CODE_OF_CONDUCT.md
â”œâ”€ .assets/
â”‚  â”œâ”€ header.png
â”‚  â””â”€ buymeacoffee.png
â”œâ”€ i18n/
â”œâ”€ public/
â”‚  â”œâ”€ font2.png
â”‚  â”œâ”€ favicon.ico
â”‚  â”œâ”€ next.svg
â”‚  â””â”€ vercel.svg
â””â”€ src/
   â”œâ”€ app/
   â”‚  â”œâ”€ page.tsx
   â”‚  â”œâ”€ layout.tsx
   â”‚  â”œâ”€ globals.css
   â”‚  â”œâ”€ button.css
   â”‚  â””â”€ api/
   â”‚     â”œâ”€ chat/route.ts
   â”‚     â””â”€ speechToText/route.ts
   â””â”€ components/
      â””â”€ AssistantButton/
         â””â”€ AssistantButton.tsx
```

## Voraussetzungen

- Node.js 18+ (empfohlen: Node.js 18.17+ oder 20 LTS fÃ¼r Next.js 13).
- npm (das Projekt nutzt `package-lock.json`).
- OpenAI API key.
- ElevenLabs API key und Voice ID.
- Ein Desktop-Browser mit Mikrofonzugriff (die mobile UX ist aktuell designbedingt eingeschrÃ¤nkt).

## Installation

1. Repository klonen:

```sh
git clone https://github.com/ntegrals/aura-voice
```

2. API-Keys von [https://openai.com/](https://openai.com/) und [https://elevenlabs.com/](https://elevenlabs.com/) holen.

Die Datei `.env.example` nach `.env.local` kopieren und deine Keys eintragen:

```sh
cp .env.example .env.local
```

```sh
OPENAI_API_KEY="YOUR OPENAI API KEY"
OPENAI_BASE_URL=(Optional)
NEXT_PUBLIC_ELEVENLABS_API_KEY="YOUR ELEVENLABS API KEY"
NEXT_PUBLIC_ELEVENLABS_VOICE_ID="YOUR ELEVENLABS VOICE ID"
```

3. AbhÃ¤ngigkeiten installieren:

```sh
npm install
```

4. App lokal starten:

```sh
npm run dev
```

5. Auf Vercel deployen:

Dieses Projekt ist mit dem standardmÃ¤ÃŸigen Vercel-Deployment-Flow fÃ¼r Next.js kompatibel.

## Konfiguration

Von diesem Projekt verwendete Umgebungsvariablen:

| Variable | Erforderlich | Beschreibung |
| --- | --- | --- |
| `OPENAI_API_KEY` | Ja | API key fÃ¼r Whisper-Transkription und GPT-Chat-Completion. |
| `OPENAI_BASE_URL` | Nein | Optionale Ãœberschreibung der OpenAI-API-Basis-URL (Proxy/Gateway). |
| `NEXT_PUBLIC_ELEVENLABS_API_KEY` | Ja | ElevenLabs API key, der fÃ¼r die browserseitige TTS-Anfrage genutzt wird. |
| `NEXT_PUBLIC_ELEVENLABS_VOICE_ID` | Ja | ElevenLabs Voice ID fÃ¼r TTS-Synthese. |

Hinweise:

- `NEXT_PUBLIC_*`-Variablen werden gemÃ¤ÃŸ Next.js-Konventionen fÃ¼r den Client verfÃ¼gbar gemacht.
- `speechToText` schreibt derzeit temporÃ¤res Audio nach `/tmp/input.webm`, bevor transkribiert wird.

## Verwendung

1. App in einem Desktop-Browser Ã¶ffnen.
2. Einmal auf die Assistenten-Kugel klicken und Mikrofonberechtigungen gewÃ¤hren.
3. Erneut klicken, um die Aufnahme zu starten, dann noch einmal klicken, um zu stoppen und zu senden.
4. Aura transkribiert deine Eingabe, erzeugt eine Antwort und spielt anschlieÃŸend synthetisierte Sprache ab.

Lokale Skripte:

```sh
npm run dev
npm run build
npm run start
npm run lint
```

## API-Beispiele

Diese Beispiele sind nÃ¼tzlich zum Debuggen lokaler API-Routen.

### `POST /api/speechToText`

```bash
curl -X POST http://localhost:3000/api/speechToText \
  -H "Content-Type: application/json" \
  -d '{"audio":"<base64-webm-audio>"}'
```

Erwartete Antwortstruktur:

```json
{
  "result": "transcribed text"
}
```

### `POST /api/chat`

```bash
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hello Aura"}]}'
```

Erwartete Antwortstruktur:

```json
"Assistant response text"
```

## Entwicklungshinweise

- Die Chat-Route ist fÃ¼r Edge Runtime konfiguriert (`export const runtime = "edge"`).
- Die Whisper-Route lÃ¤uft serverseitig und hÃ¤ngt fÃ¼r temporÃ¤re Speicherung vom Dateisystemzugriff ab.
- Die UI zeigt aktuell eine Mobile-Fallback-Nachricht statt einer vollstÃ¤ndigen mobilen Interaktion.
- Toast-Benachrichtigungen zeigen die ZustÃ¤nde Berechtigung/ZuhÃ¶ren/Nachdenken.
- Das aktuelle Prompt-Shaping fordert prÃ¤gnante Antworten (`Your answer has to be as consise as possible.`).

## Fehlerbehebung

### Mikrofon-Berechtigungsabfrage erscheint nicht

- Stelle sicher, dass dein Browser Mikrofonzugriff fÃ¼r `localhost` erlaubt.
- Nutze HTTPS beim Testen auf Nicht-`localhost`-Domains.

### Keine Audio-Wiedergabe

- PrÃ¼fe `NEXT_PUBLIC_ELEVENLABS_API_KEY` und `NEXT_PUBLIC_ELEVENLABS_VOICE_ID`.
- PrÃ¼fe Browser-BeschrÃ¤nkungen fÃ¼r Autoplay/AudioContext (Nutzerinteraktion ist erforderlich).

### API 500 von `/api/speechToText`

- PrÃ¼fe, ob `OPENAI_API_KEY` gesetzt ist.
- Stelle sicher, dass die Eingabe gÃ¼ltiges base64-kodiertes `webm`-Audio enthÃ¤lt.

### API 500 von `/api/chat`

- PrÃ¼fe, ob `OPENAI_API_KEY` und optional `OPENAI_BASE_URL` korrekt sind.
- PrÃ¼fe die ModellverfÃ¼gbarkeit fÃ¼r `gpt-4o` in deinem OpenAI-Konto.

### Hohe Latenz

- TTS-Synthesezeit dominiert normalerweise die Ende-zu-Ende-Latenz.
- Halte Prompts kurz und erwÃ¤ge, lange Antworten aufzuteilen.

## Roadmap

Potenzielle nÃ¤chste Verbesserungen, abgeleitet aus aktuellem Code und Notizen:

- Mobile-First-Interaktionssupport (ersetzt die aktuelle Desktop-Only-Begrenzung).
- Streaming partieller Assistenten-Antworten zur Reduktion der wahrgenommenen Latenz.
- Bessere Retry-/Error-UX bei Transkriptions- und TTS-Fehlern.
- Automatisierte Tests und CI-Checks ergÃ¤nzen.
- Mehrsprachige Dokumentation unter [`/i18n`](./i18n/) erweitern.

## Mitwirken

BeitrÃ¤ge sind willkommen und geschÃ¤tzt.

- Lies [CONTRIBUTING.md](./CONTRIBUTING.md) fÃ¼r Workflow und Erwartungen.
- Lies [CODE_OF_CONDUCT.md](./CODE_OF_CONDUCT.md), bevor du mitwirkst.
- ErÃ¶ffne Issues fÃ¼r Bugs oder Feature-Ideen:
- Bug report: [template](https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=bug&projects=&template=bug_report.md&title=)
- Feature request: [template](https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.md&title=)

## Kontakt

Hi! Danke, dass du diese Library ansiehst und nutzt. Wenn du dein Projekt besprechen mÃ¶chtest, Mentoring brauchst, eine Zusammenarbeit erwÃ¤gst oder einfach chatten willst, freue ich mich auf den Austausch.

Du kannst mir eine E-Mail senden: `j.schoen@mail.com` oder mir auf Twitter schreiben: [@julianschoen](https://twitter.com/julianschoen)

Wenn du etwas zurÃ¼ckgeben mÃ¶chtest, habe ich einen Buy Me A Coffee Account:

<a href="https://www.buymeacoffee.com/ntegrals">
<img src=".assets/buymeacoffee.png" alt="buymeacoffee" width="192">
</a>

Danke und hab einen groÃŸartigen Tag ğŸ‘‹

## Haftungsausschluss

Voice Assistant ist eine experimentelle Anwendung und wird "wie besehen" ohne jegliche ausdrÃ¼ckliche oder stillschweigende GewÃ¤hr bereitgestellt. Mit der Nutzung dieser Software erklÃ¤rst du dich damit einverstanden, alle mit der Nutzung verbundenen Risiken zu tragen, einschlieÃŸlich, aber nicht beschrÃ¤nkt auf Datenverlust, SystemausfÃ¤lle oder andere Probleme, die auftreten kÃ¶nnen.

Die Entwickler und Mitwirkenden dieses Projekts Ã¼bernehmen keine Verantwortung oder Haftung fÃ¼r Verluste, SchÃ¤den oder sonstige Folgen, die durch die Nutzung dieser Software entstehen kÃ¶nnen. Du bist allein verantwortlich fÃ¼r alle Entscheidungen und Handlungen, die auf Basis der von Voice Assistant bereitgestellten Informationen getroffen werden.

Bitte beachte, dass die Nutzung des GPT-4-Sprachmodells aufgrund des Token-Verbrauchs teuer sein kann. Durch die Nutzung dieses Projekts bestÃ¤tigst du, dass du selbst fÃ¼r die Ãœberwachung und Verwaltung deines Token-Verbrauchs und der damit verbundenen Kosten verantwortlich bist. Es wird dringend empfohlen, deine OpenAI-API-Nutzung regelmÃ¤ÃŸig zu prÃ¼fen und notwendige Limits oder Alerts einzurichten, um unerwartete Kosten zu vermeiden.

Mit der Nutzung von Voice Assistant erklÃ¤rst du dich einverstanden, die Entwickler, Mitwirkenden und alle verbundenen Parteien von sÃ¤mtlichen AnsprÃ¼chen, SchÃ¤den, Verlusten, Verbindlichkeiten, Kosten und Aufwendungen (einschlieÃŸlich angemessener AnwaltsgebÃ¼hren) freizustellen, zu verteidigen und schadlos zu halten, die aus deiner Nutzung dieser Software oder aus einem VerstoÃŸ gegen diese Bedingungen entstehen.

<!-- LICENSE -->

## Lizenz

VerÃ¶ffentlicht unter der MIT-Lizenz. Weitere Informationen unter `LICENSE`.

Hinweis zum Repository: Dieses Repository speichert die Lizenzdatei derzeit als [`LICENCE`](./LICENCE).
