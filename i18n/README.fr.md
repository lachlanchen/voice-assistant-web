[English](../README.md) Â· [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](README.ar.md) Â· [EspaÃ±ol](README.es.md) Â· [FranÃ§ais](README.fr.md) Â· [æ—¥æœ¬èª](README.ja.md) Â· [í•œêµ­ì–´](README.ko.md) Â· [Tiáº¿ng Viá»‡t](README.vi.md) Â· [ä¸­æ–‡ (ç®€ä½“)](README.zh-Hans.md) Â· [ä¸­æ–‡ï¼ˆç¹é«”ï¼‰](README.zh-Hant.md) Â· [Deutsch](README.de.md) Â· [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README.ru.md)


[![LazyingArt banner](https://github.com/lachlanchen/lachlanchen/raw/main/figs/banner.png)](https://github.com/lachlanchen/lachlanchen/blob/main/figs/banner.png)

![GitHub last commit](https://img.shields.io/github/last-commit/ntegrals/aura-voice?style=for-the-badge&logo=github&logoColor=white&color=0EA5E9)
![Node.js](https://img.shields.io/badge/Node.js-18%2B-10B981?style=for-the-badge&logo=nodedotjs&logoColor=white)
![TypeScript](https://img.shields.io/badge/TypeScript-5.1-3178C6?style=for-the-badge&logo=typescript&logoColor=white)
![GitHub stars](https://img.shields.io/github/stars/ntegrals/aura-voice?style=for-the-badge&logo=github&logoColor=white&color=F59E0B)
![Open Issues](https://img.shields.io/github/issues/ntegrals/aura-voice?style=for-the-badge&logo=github&logoColor=white&color=EF4444)

<a name="readme-top"></a>

<br />
<div align="center">

# Aura

<h3 align="center">Salut, voici Aura ğŸ‘‹</h3>

<p align="center">
Aura est un assistant vocal de type Siri, fonctionnant dans le navigateur, optimisÃ© pour des rÃ©ponses Ã  faible latence. Il utilise Vercel Edge Functions, la reconnaissance vocale Whisper, GPT-4o et le streaming TTS d'ElevenLabs.
<br />
<br />
<a href="https://voice.julianschoen.co"><img src="https://img.shields.io/badge/â–¶_Live_Demo-0EA5E9?style=for-the-badge&logo=googlechrome&logoColor=white" alt="Live Demo"/></a>
<a href="https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=bug&projects=&template=bug_report.md&title="><img src="https://img.shields.io/badge/ğŸ_Report_Bug-F43F5E?style=for-the-badge&logo=github&logoColor=white" alt="Report Bug"/></a>
<a href="https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.md&title="><img src="https://img.shields.io/badge/ğŸ’¡_Request_Feature-22C55E?style=for-the-badge&logo=github&logoColor=white" alt="Request Feature"/></a>
</p>

<p align="center">
<a href="https://github.com/ntegrals/aura-voice"><img alt="Repo" src="https://img.shields.io/badge/GitHub-ntegrals%2Faura--voice-181717?logo=github" /></a>
<a href="https://nextjs.org/"><img alt="Next.js" src="https://img.shields.io/badge/Next.js-13.4.13-black?logo=next.js" /></a>
<a href="https://www.typescriptlang.org/"><img alt="TypeScript" src="https://img.shields.io/badge/TypeScript-5.1-3178C6?logo=typescript&logoColor=white" /></a>
<a href="https://openai.com/"><img alt="OpenAI" src="https://img.shields.io/badge/OpenAI-GPT--4o%20%2B%20Whisper-10A37F" /></a>
<a href="https://elevenlabs.io/"><img alt="ElevenLabs" src="https://img.shields.io/badge/ElevenLabs-TTS%20Streaming-222222" /></a>
<a href="https://vercel.com/"><img alt="Vercel Edge" src="https://img.shields.io/badge/Vercel-Edge%20Runtime-000000?logo=vercel" /></a>
<a href="./LICENCE"><img alt="License" src="https://img.shields.io/badge/License-MIT-22C55E.svg" /></a>
</p>

</div>

<a href="https://github.com/ntegrals/aura-voice">
<img src=".assets//header.png" alt="Logo">
</a>

## Table des matiÃ¨res

- [ğŸ“Œ Vue d'ensemble](#overview)
- [âœ¨ FonctionnalitÃ©s](#features)
- [ğŸ¥ DÃ©mo](#demo)
- [ğŸ§  Motivation](#motivation)
- [â±ï¸ RÃ©flexions sur la latence et l'expÃ©rience utilisateur](#latence-et-experience-utilisateur)
- [ğŸ—ï¸ Architecture](#architecture)
- [ğŸ“ Structure du projet](#project-structure)
- [âœ… PrÃ©requis](#prerequisites)
- [ğŸ§° Installation](#installation)
- [âš™ï¸ Configuration](#configuration)
- [ğŸ§ª Utilisation](#utilisation)
- [ğŸ“¦ Exemples d'API](#api-examples)
- [ğŸ› ï¸ Notes de dÃ©veloppement](#development-notes)
- [ğŸ§¯ DÃ©pannage](#troubleshooting)
- [ğŸ—ºï¸ Feuille de route](#roadmap)
- [ğŸ¤ Contribution](#contributing)
- [â¤ï¸ Support](#support)
- [ğŸ“¬ Contact](#contact)
- [âš ï¸ Avertissement](#disclaimer)
- [ğŸ“„ Licence](#license)

## Vue d'ensemble <a id="overview"></a>

Aura est un assistant vocal de type Siri, basÃ© sur le navigateur et construit avec Next.js (App Router) et TypeScript.

### En bref

| Domaine | DÃ©tails |
| --- | --- |
| Objectif principal | Interaction vocale rapide, pratique et Ã  faible latence sur le web |
| ModÃ¨le d'exÃ©cution | Capture dans le navigateur + routes API serveur + endpoint de chat Edge |
| Reconnaissance vocale | OpenAI Whisper (`whisper-1`) |
| ModÃ¨le d'assistant | OpenAI GPT-4o |
| SynthÃ¨se vocale | Lecture en streaming ElevenLabs dans le navigateur |

La boucle d'interaction est :

1. Capturer l'audio du microphone dans le navigateur.
2. Transcrire la parole avec OpenAI Whisper (`whisper-1`).
3. GÃ©nÃ©rer une rÃ©ponse concise avec OpenAI GPT-4o.
4. Renvoyer l'audio synthÃ©tique en streaming vers l'utilisateur via ElevenLabs.

Le projet est optimisÃ© autour d'une expÃ©rience UX Ã  faible latence, avec un retour visuel pendant l'Ã©coute ou la rÃ©flexion de l'assistant.

### RÃ©sumÃ© visuel

| Ã‰tape | Intention |
| --- | --- |
| ğŸ™ï¸ Capture | Capture audio navigateur + Ã©tats d'interface selon la permission |
| ğŸ§  Traitement | Transcription Whisper + gÃ©nÃ©ration de rÃ©ponse GPT-4o |
| ğŸ”‰ Livraison | Lecture streaming ElevenLabs avec retour d'Ã©tat |

## FonctionnalitÃ©s <a id="features"></a>

| Fonction | Signification |
| --- | --- |
| âœ… Assistant navigateur type Siri | Interaction complÃ¨te voix vers voix dans une interface web simple |
| âš¡ Parcours Ã  faible latence | Capture, transcription, completion et lecture optimisÃ©es |
| ğŸ§  Stack LLM + TTS | OpenAI Whisper, GPT-4o et synthÃ¨se streaming ElevenLabs |
| ğŸ§© Architecture d'application extensible | Remplacer l'endpoint du modÃ¨le ou le fournisseur de voix avec des changements au niveau projet |

DÃ©tails d'implÃ©mentation supplÃ©mentaires :

| Axe | Comportement actuel |
| --- | --- |
| Framework | Next.js 13 App Router avec TypeScript |
| Runtime API | Endpoint de chat en runtime Edge (`/api/chat`) |
| Retour UX | Notifications toast pour les Ã©tats de permission micro, Ã©coute et traitement |
| Interface d'interaction | Bouton assistant animÃ© avec lecture TTS en streaming |
| RÃ©seau | Remplacement facultatif de l'URL de base OpenAI pour des configurations proxy/self-hosted |

## DÃ©mo <a id="demo"></a>

Vous pouvez tester Aura ici : [https://voice.julianschoen.co](https://voice.julianschoen.co)

## Motivation <a id="motivation"></a>

Les assistants vocaux font dÃ©sormais partie du quotidien : tÃ©lÃ©phones, voitures, maisons et plus encore. Reproduire cette expÃ©rience sur le web avec une bonne rÃ©activitÃ© a Ã©tÃ© historiquement difficile.

Jusqu'Ã  rÃ©cemment, le principal problÃ¨me des assistants vocaux sur le web Ã©tait la latence. Il fallait trop de temps pour envoyer l'audio au serveur, gÃ©nÃ©rer une completion LLM et renvoyer la parole. Les progrÃ¨s rÃ©cents d'OpenAI, ElevenLabs et Vercel ont permis de construire un assistant vocal suffisamment rapide pour Ãªtre pratique sur le web.

Ce dÃ©pÃ´t vise Ã  Ãªtre une rÃ©fÃ©rence pour les personnes qui souhaitent crÃ©er leur propre assistant vocal et comprendre les compromis des implÃ©mentations rÃ©elles.

## RÃ©flexions sur la latence et l'expÃ©rience utilisateur <a id="latence-et-experience-utilisateur"></a>

La latence est le facteur le plus important pour une bonne UX vocale. Actuellement, trois grandes sources contribuent :

- Temps de transcription (reconnaissance vocale Whisper).
- Temps de gÃ©nÃ©ration de rÃ©ponse (GPT-4o Mini dans les notes du projet original).
- Temps de streaming de la synthÃ¨se vocale (ElevenLabs TTS).

D'aprÃ¨s les notes de tests pratiques, la gÃ©nÃ©ration vocale prend gÃ©nÃ©ralement le plus de temps et est la moins prÃ©visible, notamment pour les rÃ©ponses longues.

Une stratÃ©gie d'attÃ©nuation consiste Ã  dÃ©couper la rÃ©ponse en plusieurs parties et Ã  les streamer successivement. Cela permet Ã  l'utilisateur de commencer Ã  Ã©couter plus tÃ´t tandis que le reste est encore en cours de gÃ©nÃ©ration. Ce n'est pas encore implÃ©mentÃ©, mais c'est une piste prometteuse.

Un autre concept clÃ© est le temps d'attente perÃ§u. MÃªme quand la latence totale reste fixe, les utilisateurs tolÃ¨rent mieux les dÃ©lais lorsqu'ils reÃ§oivent un retour immÃ©diat. Le projet inclut actuellement une notification de "rÃ©flexion" pendant le traitement pour amÃ©liorer la rÃ©activitÃ© perÃ§ue.

## Architecture <a id="architecture"></a>

```text
Browser (MediaRecorder)
  -> POST /api/speechToText (OpenAI Whisper transcription)
  -> POST /api/chat (OpenAI GPT-4o, Edge runtime)
  -> ElevenLabs TTS stream playback in browser (AudioContext)
```

Fichiers principaux :

- `src/components/AssistantButton/AssistantButton.tsx`: Ã©tat d'enregistrement, orchestration des requÃªtes, lecture.
- `src/app/api/speechToText/route.ts`: audio base64 -> `/tmp/input.webm` -> transcription Whisper.
- `src/app/api/chat/route.ts`: completion de chat via OpenAI.
- `src/app/page.tsx`: interface desktop-first et message de secours mobile.

## Structure du projet <a id="project-structure"></a>

```text
voice-assistant-web/
â”œâ”€ README.md
â”œâ”€ .env.example
â”œâ”€ package.json
â”œâ”€ LICENCE
â”œâ”€ CONTRIBUTING.md
â”œâ”€ CODE_OF_CONDUCT.md
â”œâ”€ .assets/
â”‚  â”œâ”€ header.png
â”‚  â””â”€ buymeacoffee.png
â”œâ”€ i18n/
â”œâ”€ public/
â”‚  â”œâ”€ font2.png
â”‚  â”œâ”€ favicon.ico
â”‚  â”œâ”€ next.svg
â”‚  â””â”€ vercel.svg
â””â”€ src/
   â”œâ”€ app/
   â”‚  â”œâ”€ page.tsx
   â”‚  â”œâ”€ layout.tsx
   â”‚  â”œâ”€ globals.css
   â”‚  â”œâ”€ button.css
   â”‚  â””â”€ api/
   â”‚     â”œâ”€ chat/route.ts
   â”‚     â””â”€ speechToText/route.ts
   â””â”€ components/
      â””â”€ AssistantButton/
         â””â”€ AssistantButton.tsx
```

## PrÃ©requis <a id="prerequisites"></a>

| Besoin | DÃ©tails |
| --- | --- |
| Node.js | 18+ (recommandÃ© : Node.js 18.17+ ou Node.js 20 LTS pour Next.js 13) |
| Gestionnaire de paquets | npm (le projet utilise `package-lock.json`) |
| AccÃ¨s API | ClÃ© API OpenAI |
| AccÃ¨s TTS | ClÃ© API ElevenLabs et ID de voix |
| Client | Navigateur desktop avec accÃ¨s micro (l'UX mobile est actuellement orientÃ©e desktop) |

## Installation <a id="installation"></a>

1. Cloner le dÃ©pÃ´t :

```sh
git clone https://github.com/ntegrals/aura-voice
```

2. Copier le modÃ¨le d'environnement et modifier les valeurs :

```sh
cp .env.example .env.local
```

```sh
OPENAI_API_KEY="YOUR OPENAI API KEY"
OPENAI_BASE_URL="" # Optional
NEXT_PUBLIC_ELEVENLABS_API_KEY="YOUR ELEVENLABS API KEY"
NEXT_PUBLIC_ELEVENLABS_VOICE_ID="YOUR ELEVENLABS VOICE ID"
```

3. Installer les dÃ©pendances :

```sh
npm install
```

4. Lancer l'application en local :

```sh
npm run dev
```

5. Ouvrir l'application sur `http://localhost:3000`.

HypothÃ¨se : si vous testez l'accÃ¨s au micro sur des domaines non locaux, le HTTPS est gÃ©nÃ©ralement requis.

6. DÃ©ployer sur Vercel :

Ce projet suit un flux de dÃ©ploiement Next.js standard. Utilisez les paramÃ¨tres d'import par dÃ©faut de Vercel et dÃ©finissez les mÃªmes variables d'environnement dans votre projet.

## Configuration <a id="configuration"></a>

Variables d'environnement utilisÃ©es par ce projet :

| Variable | Obligatoire | Description |
| --- | --- | --- |
| `OPENAI_API_KEY` | Oui | ClÃ© API utilisÃ©e pour la transcription Whisper et la completion de chat GPT. |
| `OPENAI_BASE_URL` | Non | Remplacement optionnel de l'URL de base de l'API OpenAI (proxy/gateway). |
| `NEXT_PUBLIC_ELEVENLABS_API_KEY` | Oui | ClÃ© API ElevenLabs utilisÃ©e dans la demande TTS cÃ´tÃ© navigateur. |
| `NEXT_PUBLIC_ELEVENLABS_VOICE_ID` | Oui | ID de voix ElevenLabs pour la synthÃ¨se TTS. |

Notes :

- Les variables `NEXT_PUBLIC_*` sont exposÃ©es au client par convention Next.js.
- `speechToText` Ã©crit actuellement un fichier audio temporaire dans `/tmp/input.webm` avant la transcription.

## Utilisation <a id="utilisation"></a>

1. Ouvrir l'application dans un navigateur desktop.
2. Cliquer une fois sur l'orbe d'assistant et accorder la permission microphone.
3. Cliquer Ã  nouveau pour dÃ©marrer l'enregistrement, puis Ã  nouveau pour arrÃªter et envoyer.
4. Aura transcrit votre entrÃ©e, gÃ©nÃ¨re une rÃ©ponse, puis lit la voix synthÃ©tique.

Scripts locaux :

```sh
npm run dev
npm run build
npm run start
npm run lint
```

## Exemples d'API <a id="api-examples"></a>

Ces exemples sont utiles pour dÃ©boguer les routes API locales.

### `POST /api/speechToText`

```bash
curl -X POST http://localhost:3000/api/speechToText \
  -H "Content-Type: application/json" \
  -d '{"audio":"<base64-webm-audio>"}'
```

Format de rÃ©ponse attendu :

```json
{
  "result": "transcribed text"
}
```

### `POST /api/chat`

```bash
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hello Aura"}]}'
```

Forme de rÃ©ponse attendue :

```json
"Assistant response text"
```

## Notes de dÃ©veloppement <a id="development-notes"></a>

- La route chat est configurÃ©e pour le runtime Edge (`export const runtime = "edge"`).
- La route Whisper s'exÃ©cute cÃ´tÃ© serveur et dÃ©pend de l'accÃ¨s au systÃ¨me de fichiers pour le stockage temporaire.
- L'interface affiche actuellement un message de fallback mobile plutÃ´t qu'une interaction mobile complÃ¨te.
- Les notifications toast sont utilisÃ©es pour afficher les Ã©tats de permission, d'Ã©coute et de rÃ©flexion.
- Le prompt actuel demande des rÃ©ponses concises (`Your answer has to be as concise as possible.`).
- Les logs runtime, la traÃ§abilitÃ© des requÃªtes et le comportement streaming ne sont pas encore testÃ©s en CI (pas de suite de tests automatisÃ©e dans le dÃ©pÃ´t).

## DÃ©pannage <a id="troubleshooting"></a>

### ğŸ¤ Le pop-up de permission micro n'apparaÃ®t pas

- VÃ©rifiez que votre navigateur autorise l'accÃ¨s au microphone pour `localhost`.
- Utilisez HTTPS lors de tests sur des domaines non localhost.

### ğŸ”ˆ Aucune lecture audio

- VÃ©rifiez `NEXT_PUBLIC_ELEVENLABS_API_KEY` et `NEXT_PUBLIC_ELEVENLABS_VOICE_ID`.
- VÃ©rifiez les restrictions de lecture automatique/audio-context du navigateur (une interaction utilisateur est requise).

### ğŸ“¡ Erreur 500 de l'API `/api/speechToText`

- VÃ©rifiez que `OPENAI_API_KEY` est bien dÃ©fini.
- Confirmez que l'entrÃ©e contient un audio `webm` valide encodÃ© en base64.

### ğŸ“¡ Erreur 500 de l'API `/api/chat`

- VÃ©rifiez que `OPENAI_API_KEY` et Ã©ventuellement `OPENAI_BASE_URL` sont corrects.
- VÃ©rifiez la disponibilitÃ© du modÃ¨le `gpt-4o` dans votre compte OpenAI.

### â³ Haute latence

- Le temps de synthÃ¨se TTS domine gÃ©nÃ©ralement la latence de bout en bout.
- Gardez les prompts concis et envisagez de dÃ©couper les rÃ©ponses longues.

## Feuille de route <a id="roadmap"></a>

AmÃ©liorations potentielles dÃ©duites du code actuel et des notes :

- Prise en charge d'interaction orientÃ©e mobile (remplacement du filtrage desktop-only actuel).
- Diffusion partielle des rÃ©ponses de l'assistant en streaming pour rÃ©duire la latence perÃ§ue.
- Meilleure UX de retry/erreur autour des Ã©checs de transcription et de TTS.
- Ajout de tests automatisÃ©s et de contrÃ´les CI.
- Extension de la documentation multilingue sous [`/i18n`](./i18n/).

## Contribution <a id="contributing"></a>

Les contributions sont les bienvenues et apprÃ©ciÃ©es.

- Consultez [CONTRIBUTING.md](./CONTRIBUTING.md) pour le flux de travail et les attentes.
- Consultez [CODE_OF_CONDUCT.md](./CODE_OF_CONDUCT.md) avant de participer.
- Ouvrez un ticket pour les bugs ou idÃ©es de fonctionnalitÃ©s :
- Rapport de bug : [template](https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=bug&projects=&template=bug_report.md&title=)
- Demande de fonctionnalitÃ© : [template](https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.md&title=)

## â¤ï¸ Support

| Donate | PayPal | Stripe |
| --- | --- | --- |
| [![Donate](https://camo.githubusercontent.com/24a4914f0b42c6f435f9e101621f1e52535b02c225764b2f6cc99416926004b7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d4c617a79696e674172742d3045413545393f7374796c653d666f722d7468652d6261646765266c6f676f3d6b6f2d6669266c6f676f436f6c6f723d7768697465)](https://chat.lazying.art/donate) | [![PayPal](https://camo.githubusercontent.com/d0f57e8b016517a4b06961b24d0ca87d62fdba16e18bbdb6aba28e978dc0ea21/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617950616c2d526f6e677a686f754368656e2d3030343537433f7374796c653d666f722d7468652d6261646765266c6f676f3d70617970616c266c6f676f436f6c6f723d7768697465)](https://paypal.me/RongzhouChen) | [![Stripe](https://camo.githubusercontent.com/1152dfe04b6943afe3a8d2953676749603fb9f95e24088c92c97a01a897b4942/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5374726970652d446f6e6174652d3633354246463f7374796c653d666f722d7468652d6261646765266c6f676f3d737472697065266c6f676f436f6c6f723d7768697465)](https://buy.stripe.com/aFadR8gIaflgfQV6T4fw400) |

## Contact <a id="contact"></a>

Bonjour ! Merci d'avoir regardÃ© et utilisÃ© cette bibliothÃ¨que. Si vous Ãªtes intÃ©ressÃ© par une discussion sur votre projet, que vous cherchez un mentorat, souhaitez m'engager, ou simplement discuter, je suis heureux de vous parler.

Vous pouvez m'envoyer un e-mail : `j.schoen@mail.com` ou me contacter sur Twitter : [@julianschoen](https://twitter.com/julianschoen)

Si vous souhaitez offrir un soutien, j'ai un compte Buy Me A Coffee :

<a href="https://www.buymeacoffee.com/ntegrals">
<img src=".assets/buymeacoffee.png" alt="buymeacoffee" width="192">
</a>

Merci et bonne journÃ©e ğŸ‘‹

## Disclaimer <a id="disclaimer"></a>

Voice Assistant est une application expÃ©rimentale et est fournie Â« en l'Ã©tat Â», sans aucune garantie, explicite ou implicite. En utilisant ce logiciel, vous acceptez d'assumer tous les risques liÃ©s Ã  son utilisation, notamment, sans limitation, la perte de donnÃ©es, la dÃ©faillance du systÃ¨me ou d'autres problÃ¨mes pouvant survenir.

Les dÃ©veloppeurs et contributeurs de ce projet n'acceptent aucune responsabilitÃ© pour les pertes, dommages ou consÃ©quences pouvant dÃ©couler de l'utilisation de ce logiciel. Vous Ãªtes seul responsable des dÃ©cisions et actions prises sur la base des informations fournies par Voice Assistant.

Veuillez noter que l'utilisation du modÃ¨le GPT-4 peut Ãªtre coÃ»teuse en raison de la consommation de tokens. En utilisant ce projet, vous reconnaissez Ãªtre responsable de surveiller et de gÃ©rer votre propre consommation de tokens et vos coÃ»ts associÃ©s. Il est vivement recommandÃ© de consulter rÃ©guliÃ¨rement votre usage de l'API OpenAI et de mettre en place les limites ou alertes nÃ©cessaires pour Ã©viter des frais inattendus.

En utilisant Voice Assistant, vous acceptez d'indemniser, de dÃ©fendre et d'exonÃ©rer les dÃ©veloppeurs, contributeurs et toute partie affiliÃ©e de toute rÃ©clamation, dommage, perte, responsabilitÃ©, coÃ»t et dÃ©pense (y compris les honoraires raisonnables d'avocats) dÃ©coulant de votre utilisation de ce logiciel ou de la violation de ces conditions.

<!-- LICENSE -->

## License <a id="license"></a>

Distributed under the MIT License. See `LICENSE` for more information.

Repository note: this repository currently stores the license file as [`LICENCE`](./LICENCE).
