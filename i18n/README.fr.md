[English](../README.md) Â· [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](README.ar.md) Â· [EspaÃ±ol](README.es.md) Â· [FranÃ§ais](README.fr.md) Â· [æ—¥æœ¬èª](README.ja.md) Â· [í•œêµ­ì–´](README.ko.md) Â· [Tiáº¿ng Viá»‡t](README.vi.md) Â· [ä¸­æ–‡ (ç®€ä½“)](README.zh-Hans.md) Â· [ä¸­æ–‡ï¼ˆç¹é«”ï¼‰](README.zh-Hant.md) Â· [Deutsch](README.de.md) Â· [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README.ru.md)


<a name="readme-top"></a>

<br />
<div align="center">

<h3 align="center">Dites bonjour Ã  Aura ğŸ‘‹</h3>

<p align="center">
Aura est un assistant vocal intelligent optimisÃ© pour des rÃ©ponses Ã  faible latence. Il utilise les Edge Functions de Vercel, la reconnaissance vocale Whisper, GPT-4o et le streaming TTS d'ElevenLabs.
<br />
<br />
<a href="https://voice.julianschoen.co">Voir la dÃ©mo</a>
Â·
<a href="https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=bug&projects=&template=bug_report.md&title=">Signaler un bug</a>
Â·
<a href="https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.md&title=">Demander une fonctionnalitÃ©</a>
</p>

<p align="center">
<a href="https://github.com/ntegrals/aura-voice"><img alt="Repo" src="https://img.shields.io/badge/GitHub-ntegrals%2Faura--voice-181717?logo=github" /></a>
<a href="https://nextjs.org/"><img alt="Next.js" src="https://img.shields.io/badge/Next.js-13.4.13-black?logo=next.js" /></a>
<a href="https://www.typescriptlang.org/"><img alt="TypeScript" src="https://img.shields.io/badge/TypeScript-5.1-3178C6?logo=typescript&logoColor=white" /></a>
<a href="https://openai.com/"><img alt="OpenAI" src="https://img.shields.io/badge/OpenAI-GPT--4o%20%2B%20Whisper-10A37F" /></a>
<a href="https://elevenlabs.io/"><img alt="ElevenLabs" src="https://img.shields.io/badge/ElevenLabs-TTS%20Streaming-222222" /></a>
<a href="https://vercel.com/"><img alt="Vercel Edge" src="https://img.shields.io/badge/Vercel-Edge%20Runtime-000000?logo=vercel" /></a>
<a href="./LICENCE"><img alt="License" src="https://img.shields.io/badge/License-MIT-22C55E.svg" /></a>
</p>

</div>

<a href="https://github.com/ntegrals/aura-voice">
<img src=".assets//header.png" alt="Logo">
</a>

## Table des matiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [FonctionnalitÃ©s](#fonctionnalitÃ©s)
- [DÃ©mo](#dÃ©mo)
- [Motivation](#motivation)
- [RÃ©flexions sur la latence et l'expÃ©rience utilisateur](#rÃ©flexions-sur-la-latence-et-lexpÃ©rience-utilisateur)
- [Architecture](#architecture)
- [Structure du projet](#structure-du-projet)
- [PrÃ©requis](#prÃ©requis)
- [Installation](#installation)
- [Configuration](#configuration)
- [Utilisation](#utilisation)
- [Exemples d'API](#exemples-dapi)
- [Notes de dÃ©veloppement](#notes-de-dÃ©veloppement)
- [DÃ©pannage](#dÃ©pannage)
- [Feuille de route](#feuille-de-route)
- [Contribution](#contribution)
- [Contact](#contact)
- [Avertissement](#avertissement)
- [Licence](#licence)

## Vue d'ensemble

Aura est un assistant vocal de type Siri, basÃ© navigateur, construit avec Next.js (App Router) et TypeScript.

### En un coup d'Å“il

| Domaine | DÃ©tails |
| --- | --- |
| Objectif principal | Interaction vocale web rapide, pratique et Ã  faible latence |
| ModÃ¨le d'exÃ©cution | Capture navigateur + routes API serveur + endpoint chat Edge |
| Speech-to-text | OpenAI Whisper (`whisper-1`) |
| ModÃ¨le assistant | OpenAI GPT-4o |
| Text-to-speech | Lecture en streaming ElevenLabs dans le navigateur |

La boucle d'interaction est la suivante:

1. Capturer l'audio du microphone dans le navigateur.
2. Transcrire la voix avec OpenAI Whisper (`whisper-1`).
3. GÃ©nÃ©rer une rÃ©ponse concise avec OpenAI GPT-4o.
4. Diffuser l'audio synthÃ©tisÃ© vers l'utilisateur avec ElevenLabs.

Le projet est optimisÃ© pour une UX pratique Ã  faible latence, avec un retour visuel pendant que l'assistant Ã©coute ou rÃ©flÃ©chit.

## FonctionnalitÃ©s

âœ… Un assistant vocal de type Siri directement dans votre navigateur  
âœ… OptimisÃ© pour des rÃ©ponses Ã  faible latence  
âœ… Construit avec OpenAI, la reconnaissance vocale Whisper et ElevenLabs

DÃ©tails d'implÃ©mentation supplÃ©mentaires:

- Next.js 13 App Router avec TypeScript.
- Endpoint de chat Edge runtime (`/api/chat`).
- Retour d'interaction via notifications toast (permission micro, Ã©coute, rÃ©flexion).
- Bouton assistant animÃ© avec lecture TTS en streaming.
- Remplacement optionnel de l'URL de base OpenAI pour les configurations proxy/passerelle auto-hÃ©bergÃ©e.

## DÃ©mo

Vous pouvez tester Aura ici: [https://voice.julianschoen.co](https://voice.julianschoen.co)

## Motivation

Les assistants vocaux sont devenus une partie intÃ©grante du quotidien: tÃ©lÃ©phones, voitures, maisons, et plus encore. Reproduire cette expÃ©rience sur le web avec une bonne rÃ©activitÃ© a longtemps Ã©tÃ© difficile.

Jusqu'Ã  rÃ©cemment, le principal problÃ¨me des assistants vocaux sur le web Ã©tait la latence. Il fallait trop de temps pour envoyer l'audio au serveur, gÃ©nÃ©rer une complÃ©tion LLM et renvoyer l'audio en streaming. Les avancÃ©es rÃ©centes d'OpenAI, ElevenLabs et Vercel ont rendu possible la crÃ©ation d'un assistant vocal suffisamment rapide pour un usage pratique sur le web.

Ce dÃ©pÃ´t vise Ã  devenir une rÃ©fÃ©rence pour celles et ceux qui veulent crÃ©er leur propre assistant vocal et comprendre les compromis des implÃ©mentations rÃ©elles.

## RÃ©flexions sur la latence et l'expÃ©rience utilisateur

La latence est le facteur le plus important pour une bonne UX vocale. Actuellement, il y a trois contributeurs majeurs:

- Temps de transcription (reconnaissance vocale Whisper).
- Temps de gÃ©nÃ©ration de rÃ©ponse (GPT-4o Mini dans les notes du projet d'origine).
- Temps de streaming de synthÃ¨se vocale (ElevenLabs TTS).

D'aprÃ¨s les notes de tests pratiques, la gÃ©nÃ©ration vocale est gÃ©nÃ©ralement l'Ã©tape la plus longue et la moins prÃ©visible, en particulier pour les rÃ©ponses longues.

Une stratÃ©gie d'attÃ©nuation possible consiste Ã  dÃ©couper la rÃ©ponse en plusieurs parties et Ã  les diffuser l'une aprÃ¨s l'autre. Cela permet Ã  l'utilisateur de commencer Ã  Ã©couter plus tÃ´t pendant que le reste est encore en cours de gÃ©nÃ©ration. Ce n'est pas encore implÃ©mentÃ©, mais c'est une direction prometteuse.

Un autre concept clÃ© est le temps d'attente perÃ§u. MÃªme lorsque la latence totale est fixe, les utilisateurs tolÃ¨rent mieux les dÃ©lais lorsqu'ils reÃ§oivent un retour immÃ©diat. Le projet inclut actuellement une notification Â« rÃ©flexion Â» pendant le traitement afin d'amÃ©liorer la rÃ©activitÃ© perÃ§ue.

## Architecture

```text
Browser (MediaRecorder)
  -> POST /api/speechToText (OpenAI Whisper transcription)
  -> POST /api/chat (OpenAI GPT-4o, Edge runtime)
  -> ElevenLabs TTS stream playback in browser (AudioContext)
```

Fichiers clÃ©s:

- `src/components/AssistantButton/AssistantButton.tsx`: Ã©tat d'enregistrement, orchestration des requÃªtes, lecture audio.
- `src/app/api/speechToText/route.ts`: audio base64 -> `/tmp/input.webm` -> transcription Whisper.
- `src/app/api/chat/route.ts`: complÃ©tion de chat via OpenAI.
- `src/app/page.tsx`: interface orientÃ©e desktop et message de repli pour mobile.

## Structure du projet

```text
voice-assistant-web/
â”œâ”€ README.md
â”œâ”€ .env.example
â”œâ”€ package.json
â”œâ”€ LICENCE
â”œâ”€ CONTRIBUTING.md
â”œâ”€ CODE_OF_CONDUCT.md
â”œâ”€ .assets/
â”‚  â”œâ”€ header.png
â”‚  â””â”€ buymeacoffee.png
â”œâ”€ i18n/
â”œâ”€ public/
â”‚  â”œâ”€ font2.png
â”‚  â”œâ”€ favicon.ico
â”‚  â”œâ”€ next.svg
â”‚  â””â”€ vercel.svg
â””â”€ src/
   â”œâ”€ app/
   â”‚  â”œâ”€ page.tsx
   â”‚  â”œâ”€ layout.tsx
   â”‚  â”œâ”€ globals.css
   â”‚  â”œâ”€ button.css
   â”‚  â””â”€ api/
   â”‚     â”œâ”€ chat/route.ts
   â”‚     â””â”€ speechToText/route.ts
   â””â”€ components/
      â””â”€ AssistantButton/
         â””â”€ AssistantButton.tsx
```

## PrÃ©requis

- Node.js 18+ (recommandÃ©: Node.js 18.17+ ou 20 LTS pour Next.js 13).
- npm (le projet utilise `package-lock.json`).
- ClÃ© API OpenAI.
- ClÃ© API ElevenLabs et ID de voix.
- Un navigateur desktop avec accÃ¨s au microphone (l'UX mobile est actuellement limitÃ©e par conception).

## Installation

1. Cloner le dÃ©pÃ´t:

```sh
git clone https://github.com/ntegrals/aura-voice
```

2. RÃ©cupÃ©rer les clÃ©s API depuis [https://openai.com/](https://openai.com/) et [https://elevenlabs.com/](https://elevenlabs.com/).

Copiez le fichier `.env.example` vers `.env.local` et ajoutez vos clÃ©s:

```sh
cp .env.example .env.local
```

```sh
OPENAI_API_KEY="YOUR OPENAI API KEY"
OPENAI_BASE_URL=(Optional)
NEXT_PUBLIC_ELEVENLABS_API_KEY="YOUR ELEVENLABS API KEY"
NEXT_PUBLIC_ELEVENLABS_VOICE_ID="YOUR ELEVENLABS VOICE ID"
```

3. Installer les dÃ©pendances:

```sh
npm install
```

4. Lancer l'application en local:

```sh
npm run dev
```

5. DÃ©ployer sur Vercel:

Ce projet est compatible avec le flux de dÃ©ploiement standard de Vercel pour Next.js.

## Configuration

Variables d'environnement utilisÃ©es par ce projet:

| Variable | Obligatoire | Description |
| --- | --- | --- |
| `OPENAI_API_KEY` | Oui | ClÃ© API utilisÃ©e pour la transcription Whisper et la complÃ©tion de chat GPT. |
| `OPENAI_BASE_URL` | Non | Remplacement optionnel de l'URL de base de l'API OpenAI (proxy/passerelle). |
| `NEXT_PUBLIC_ELEVENLABS_API_KEY` | Oui | ClÃ© API ElevenLabs utilisÃ©e dans la requÃªte TTS cÃ´tÃ© navigateur. |
| `NEXT_PUBLIC_ELEVENLABS_VOICE_ID` | Oui | ID de voix ElevenLabs pour la synthÃ¨se TTS. |

Notes:

- Les variables `NEXT_PUBLIC_*` sont exposÃ©es au client selon les conventions Next.js.
- `speechToText` Ã©crit actuellement l'audio temporaire dans `/tmp/input.webm` avant transcription.

## Utilisation

1. Ouvrez l'application dans un navigateur desktop.
2. Cliquez une fois sur l'orbe de l'assistant et accordez les permissions micro.
3. Cliquez Ã  nouveau pour dÃ©marrer l'enregistrement, puis cliquez encore pour arrÃªter et envoyer.
4. Aura transcrit votre entrÃ©e, gÃ©nÃ¨re une rÃ©ponse, puis lit la voix synthÃ©tisÃ©e.

Scripts locaux:

```sh
npm run dev
npm run build
npm run start
npm run lint
```

## Exemples d'API

Ces exemples sont utiles pour dÃ©boguer les routes API locales.

### `POST /api/speechToText`

```bash
curl -X POST http://localhost:3000/api/speechToText \
  -H "Content-Type: application/json" \
  -d '{"audio":"<base64-webm-audio>"}'
```

Format de rÃ©ponse attendu:

```json
{
  "result": "transcribed text"
}
```

### `POST /api/chat`

```bash
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hello Aura"}]}'
```

Format de rÃ©ponse attendu:

```json
"Assistant response text"
```

## Notes de dÃ©veloppement

- La route de chat est configurÃ©e pour l'Edge runtime (`export const runtime = "edge"`).
- La route Whisper s'exÃ©cute cÃ´tÃ© serveur et dÃ©pend de l'accÃ¨s au systÃ¨me de fichiers pour le stockage temporaire.
- L'UI fournit actuellement un message de repli mobile au lieu d'une interaction mobile complÃ¨te.
- Les notifications toast sont utilisÃ©es pour afficher les Ã©tats permission/Ã©coute/rÃ©flexion.
- La formulation actuelle du prompt demande des rÃ©ponses concises (`Your answer has to be as consise as possible.`).

## DÃ©pannage

### La demande de permission micro n'apparaÃ®t pas

- Assurez-vous que votre navigateur autorise l'accÃ¨s au microphone pour `localhost`.
- Utilisez HTTPS lors des tests sur des domaines non-localhost.

### Aucun son en lecture

- VÃ©rifiez `NEXT_PUBLIC_ELEVENLABS_API_KEY` et `NEXT_PUBLIC_ELEVENLABS_VOICE_ID`.
- VÃ©rifiez les restrictions de lecture automatique/audio-context du navigateur (une interaction utilisateur est nÃ©cessaire).

### API 500 depuis `/api/speechToText`

- Confirmez que `OPENAI_API_KEY` est dÃ©fini.
- Validez que l'entrÃ©e contient un audio `webm` encodÃ© en base64 valide.

### API 500 depuis `/api/chat`

- Confirmez que `OPENAI_API_KEY` et `OPENAI_BASE_URL` (optionnel) sont corrects.
- VÃ©rifiez la disponibilitÃ© du modÃ¨le `gpt-4o` dans votre compte OpenAI.

### Latence Ã©levÃ©e

- Le temps de synthÃ¨se TTS domine gÃ©nÃ©ralement la latence de bout en bout.
- Gardez les prompts concis et envisagez de dÃ©couper les rÃ©ponses longues.

## Feuille de route

AmÃ©liorations potentielles dÃ©duites du code actuel et des notes:

- Support d'interaction mobile-first (remplacer le blocage actuel orientÃ© desktop).
- Streaming de rÃ©ponses partielles de l'assistant pour rÃ©duire la latence perÃ§ue.
- Meilleure UX de reprise/erreur autour des Ã©checs de transcription et TTS.
- Ajouter des tests automatisÃ©s et des vÃ©rifications CI.
- Ã‰tendre la documentation multilingue sous [`/i18n`](./i18n/).

## Contribution

Les contributions sont les bienvenues et apprÃ©ciÃ©es.

- Lisez [CONTRIBUTING.md](./CONTRIBUTING.md) pour le workflow et les attentes.
- Lisez [CODE_OF_CONDUCT.md](./CODE_OF_CONDUCT.md) avant de participer.
- Ouvrez des issues pour les bugs ou idÃ©es de fonctionnalitÃ©s:
- Rapport de bug: [template](https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=bug&projects=&template=bug_report.md&title=)
- Demande de fonctionnalitÃ©: [template](https://github.com/ntegrals/aura-voice/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.md&title=)

## Contact

Bonjour! Merci d'avoir consultÃ© et utilisÃ© cette bibliothÃ¨que. Si vous souhaitez discuter de votre projet, avez besoin de mentorat, envisagez de m'embaucher, ou voulez simplement Ã©changer, je serai ravi de parler avec vous.

Vous pouvez m'envoyer un e-mail: `j.schoen@mail.com` ou m'Ã©crire sur Twitter: [@julianschoen](https://twitter.com/julianschoen)

Si vous souhaitez offrir un petit soutien, j'ai un compte Buy Me A Coffee:

<a href="https://www.buymeacoffee.com/ntegrals">
<img src=".assets/buymeacoffee.png" alt="buymeacoffee" width="192">
</a>

Merci et excellente journÃ©e ğŸ‘‹

## Avertissement

Voice Assistant est une application expÃ©rimentale et est fournie Â« telle quelle Â» sans aucune garantie, expresse ou implicite. En utilisant ce logiciel, vous acceptez d'assumer tous les risques associÃ©s Ã  son utilisation, y compris, sans s'y limiter, la perte de donnÃ©es, les pannes systÃ¨me, ou tout autre problÃ¨me pouvant survenir.

Les dÃ©veloppeurs et contributeurs de ce projet n'acceptent aucune responsabilitÃ© ni obligation pour toute perte, tout dommage, ou toute autre consÃ©quence pouvant rÃ©sulter de l'utilisation de ce logiciel. Vous Ãªtes seul responsable de toute dÃ©cision et action prise sur la base des informations fournies par Voice Assistant.

Veuillez noter que l'utilisation du modÃ¨le de langage GPT-4 peut Ãªtre coÃ»teuse en raison de l'usage de tokens. En utilisant ce projet, vous reconnaissez qu'il vous incombe de surveiller et de gÃ©rer votre propre consommation de tokens ainsi que les coÃ»ts associÃ©s. Il est fortement recommandÃ© de vÃ©rifier rÃ©guliÃ¨rement votre consommation API OpenAI et de configurer les limites ou alertes nÃ©cessaires pour Ã©viter des frais inattendus.

En utilisant Voice Assistant, vous acceptez d'indemniser, de dÃ©fendre et de dÃ©gager de toute responsabilitÃ© les dÃ©veloppeurs, contributeurs et toutes les parties affiliÃ©es contre toute rÃ©clamation, tout dommage, toute perte, toute responsabilitÃ©, tout coÃ»t et toute dÃ©pense (y compris les honoraires raisonnables d'avocats) dÃ©coulant de votre utilisation de ce logiciel ou de votre violation des prÃ©sentes conditions.

<!-- LICENSE -->

## Licence

DistribuÃ© sous licence MIT. Voir `LICENSE` pour plus d'informations.

Note du dÃ©pÃ´t: ce dÃ©pÃ´t stocke actuellement le fichier de licence sous [`LICENCE`](./LICENCE).
